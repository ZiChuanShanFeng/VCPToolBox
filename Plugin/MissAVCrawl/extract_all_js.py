#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æå–é¡µé¢ä¸­çš„æ‰€æœ‰JavaScriptä»£ç ï¼ŒæŸ¥æ‰¾cdnUrlå‡½æ•°
"""

import sys
import re
import json
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from missav_api_core import MissAVCrawler

def extract_all_javascript(url):
    """æå–é¡µé¢ä¸­çš„æ‰€æœ‰JavaScriptä»£ç """
    print(f"ğŸ” æå–æ‰€æœ‰JavaScriptä»£ç : {url}")
    print("=" * 80)
    
    # è·å–é¡µé¢å†…å®¹
    crawler = MissAVCrawler()
    extractor = crawler.client.info_extractor
    content = extractor.core.fetch(url)
    
    if not content:
        print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
        return
    
    print(f"âœ… é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    print("-" * 80)
    
    # æå–æ‰€æœ‰scriptæ ‡ç­¾å†…å®¹
    script_pattern = r'<script[^>]*>(.*?)</script>'
    scripts = re.findall(script_pattern, content, re.DOTALL | re.IGNORECASE)
    
    print(f"æ‰¾åˆ° {len(scripts)} ä¸ªscriptæ ‡ç­¾")
    
    # å°†æ‰€æœ‰JavaScriptä»£ç åˆå¹¶
    all_js_code = '\n'.join(scripts)
    
    # ä¿å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
    js_file = Path("extracted_javascript.js")
    with open(js_file, 'w', encoding='utf-8') as f:
        f.write(all_js_code)
    
    print(f"âœ… JavaScriptä»£ç å·²ä¿å­˜åˆ°: {js_file}")
    print(f"âœ… æ€»JavaScriptä»£ç é•¿åº¦: {len(all_js_code)} å­—ç¬¦")
    
    # æŸ¥æ‰¾cdnUrlç›¸å…³çš„ä»£ç 
    print("\nğŸ“¥ æŸ¥æ‰¾cdnUrlç›¸å…³ä»£ç :")
    
    # åˆ†è¡ŒæŸ¥æ‰¾
    lines = all_js_code.split('\n')
    cdn_lines = []
    
    for i, line in enumerate(lines, 1):
        if 'cdnurl' in line.lower() or 'cdn_url' in line.lower() or 'cdnUrl' in line:
            clean_line = line.strip()
            if clean_line and len(clean_line) > 5:
                print(f"  è¡Œ{i}: {clean_line}")
                cdn_lines.append((i, clean_line))
    
    # æŸ¥æ‰¾å¯èƒ½çš„CDNåŸºç¡€URLå®šä¹‰
    print("\nğŸ“¥ æŸ¥æ‰¾CDNåŸºç¡€URLå®šä¹‰:")
    
    cdn_base_patterns = [
        r'["\']https://[^"\']*doppiocdn[^"\']*["\']',
        r'["\']https://media[^"\']*["\']',
        r'baseUrl\s*[:=]\s*["\'][^"\']+["\']',
        r'mediaUrl\s*[:=]\s*["\'][^"\']+["\']',
        r'MEDIA_BASE\s*[:=]\s*["\'][^"\']+["\']',
    ]
    
    found_bases = set()
    
    for pattern in cdn_base_patterns:
        matches = re.findall(pattern, all_js_code, re.IGNORECASE)
        for match in matches:
            clean_match = match.strip('"\'')
            if 'http' in clean_match:
                found_bases.add(clean_match)
    
    print("æ‰¾åˆ°çš„CDNåŸºç¡€URL:")
    for base in sorted(found_bases):
        print(f"  - {base}")
    
    # æŸ¥æ‰¾å‡½æ•°å®šä¹‰æ¨¡å¼
    print("\nğŸ“¥ æŸ¥æ‰¾å‡½æ•°å®šä¹‰:")
    
    function_patterns = [
        r'function\s+\w*[Cc]dn\w*\s*\([^)]*\)\s*{[^}]*}',
        r'\w*[Cc]dn\w*\s*[:=]\s*function\s*\([^)]*\)\s*{[^}]*}',
        r'\w*[Cc]dn\w*\s*[:=]\s*\([^)]*\)\s*=>\s*[^;]+',
        r'const\s+\w*[Cc]dn\w*\s*=\s*[^;]+',
        r'var\s+\w*[Cc]dn\w*\s*=\s*[^;]+',
        r'let\s+\w*[Cc]dn\w*\s*=\s*[^;]+',
    ]
    
    for i, pattern in enumerate(function_patterns, 1):
        matches = re.findall(pattern, all_js_code, re.IGNORECASE | re.DOTALL)
        if matches:
            print(f"å‡½æ•°æ¨¡å¼{i}: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
            for match in matches:
                clean_match = match.replace('\n', ' ').replace('\r', '')
                if len(clean_match) > 150:
                    clean_match = clean_match[:150] + "..."
                print(f"  - {clean_match}")
    
    # æŸ¥æ‰¾Alpine.jsçš„å…¨å±€æ•°æ®
    print("\nğŸ“¥ æŸ¥æ‰¾Alpine.jså…¨å±€æ•°æ®:")
    
    alpine_patterns = [
        r'Alpine\.data\s*\(\s*["\'][^"\']+["\']\s*,\s*function\s*\(\)\s*{[^}]*return\s*{[^}]*}[^}]*}',
        r'window\.\w+\s*=\s*{[^}]*}',
        r'var\s+\w+\s*=\s*{[^}]*cdnUrl[^}]*}',
    ]
    
    for i, pattern in enumerate(alpine_patterns, 1):
        matches = re.findall(pattern, all_js_code, re.IGNORECASE | re.DOTALL)
        if matches:
            print(f"Alpineæ¨¡å¼{i}: æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
            for match in matches[:2]:
                clean_match = match.replace('\n', ' ')
                if len(clean_match) > 200:
                    clean_match = clean_match[:200] + "..."
                print(f"  - {clean_match}")
    
    return cdn_lines, found_bases

def analyze_network_requests():
    """åˆ†æå¯èƒ½çš„ç½‘ç»œè¯·æ±‚æ¨¡å¼"""
    print("\nğŸ“¥ åˆ†æç½‘ç»œè¯·æ±‚æ¨¡å¼")
    print("-" * 80)
    
    # åŸºäºä½ æä¾›çš„ç¤ºä¾‹URLï¼Œåˆ†æå¯èƒ½çš„æ¨¡å¼
    example_url = "https://media-hls.doppiocdn.net/b-hls-06/117758835/117758835_240p_h264_501_FYTY49jYuOBbckr0_1754223961.mp4"
    
    print(f"ç¤ºä¾‹URL: {example_url}")
    
    # åˆ†æURLæ¨¡å¼
    # æ ¼å¼: https://media-hls.doppiocdn.net/b-hls-06/{numeric_id}/{numeric_id}_{quality}_h264_501_FYTY49jYuOBbckr0_{timestamp}.mp4
    
    # æå–æ¨¡å¼
    pattern_match = re.match(r'https://([^/]+)/([^/]+)/(\d+)/(\d+)_(\w+)_(.+)\.mp4', example_url)
    
    if pattern_match:
        domain = pattern_match.group(1)
        path_prefix = pattern_match.group(2)
        numeric_id = pattern_match.group(3)
        quality = pattern_match.group(5)
        suffix = pattern_match.group(6)
        
        print(f"åŸŸå: {domain}")
        print(f"è·¯å¾„å‰ç¼€: {path_prefix}")
        print(f"æ•°å­—ID: {numeric_id}")
        print(f"è´¨é‡: {quality}")
        print(f"åç¼€: {suffix}")
        
        # å¯¹äºjul-875ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°å¯¹åº”çš„æ•°å­—ID
        print(f"\nå¯¹äºjul-875ï¼Œéœ€è¦æ‰¾åˆ°å¯¹åº”çš„æ•°å­—ID")
        print("å¯èƒ½çš„é¢„è§ˆè§†é¢‘URLæ¨¡å¼:")
        
        # å‡è®¾é¢„è§ˆè§†é¢‘ä½¿ç”¨ç›¸åŒçš„åŸŸåå’Œè·¯å¾„ç»“æ„
        possible_patterns = [
            f"https://{domain}/{path_prefix}/{{numeric_id}}/{{numeric_id}}_preview.mp4",
            f"https://{domain}/{path_prefix}/{{numeric_id}}/preview.mp4",
            f"https://{domain}/preview/{{numeric_id}}.mp4",
            f"https://{domain}/{{numeric_id}}/preview.mp4",
        ]
        
        for pattern in possible_patterns:
            print(f"  - {pattern}")
    
    # å°è¯•ä¸€äº›å¸¸è§çš„æ•°å­—ID
    print(f"\nå°è¯•ä¸€äº›å¯èƒ½çš„æ•°å­—ID:")
    
    # è¿™äº›å¯èƒ½æ˜¯jul-875å¯¹åº”çš„æ•°å­—ID
    possible_ids = [
        "875",
        "000875",
        "0000875",
        "jul875",
        # å¯èƒ½éœ€è¦ä»é¡µé¢ä¸­æå–å®é™…çš„ID
    ]
    
    for pid in possible_ids:
        print(f"  å¯èƒ½ID: {pid}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æå–æ‰€æœ‰JavaScriptä»£ç ï¼ŒæŸ¥æ‰¾cdnUrlå‡½æ•°")
    print("=" * 80)
    
    # æµ‹è¯•URL
    test_url = "https://missav.ws/dm44/jul-875"
    
    # æå–JavaScriptä»£ç 
    cdn_lines, found_bases = extract_all_javascript(test_url)
    
    # åˆ†æç½‘ç»œè¯·æ±‚æ¨¡å¼
    analyze_network_requests()
    
    print("\n" + "=" * 80)
    print("ğŸ“‹ æå–æ€»ç»“:")
    print(f"æ‰¾åˆ° {len(cdn_lines)} è¡ŒåŒ…å«cdnUrlçš„ä»£ç ")
    print(f"æ‰¾åˆ° {len(found_bases)} ä¸ªCDNåŸºç¡€URL")
    
    if cdn_lines:
        print("\ncdnUrlç›¸å…³ä»£ç è¡Œ:")
        for line_num, line in cdn_lines[:5]:
            print(f"  è¡Œ{line_num}: {line}")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("1. æ£€æŸ¥extracted_javascript.jsæ–‡ä»¶ï¼Œæ‰‹åŠ¨æŸ¥æ‰¾cdnUrlå‡½æ•°å®šä¹‰")
    print("2. å¯èƒ½éœ€è¦ä»é¡µé¢çš„æ•°æ®ä¸­æå–å®é™…çš„æ•°å­—ID")
    print("3. å°è¯•ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·æŸ¥çœ‹ç½‘ç»œè¯·æ±‚")
    print("4. åˆ†æAlpine.jsçš„æ•°æ®ç»‘å®šé€»è¾‘")

if __name__ == "__main__":
    main()