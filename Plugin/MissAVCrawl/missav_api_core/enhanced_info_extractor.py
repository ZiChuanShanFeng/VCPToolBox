#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„ä¿¡æ¯æ£€ç´¢æ¨¡å—
æ”¯æŒæå–æ›´å¤šè§†é¢‘ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¸‹è½½åˆ†è¾¨ç‡ã€è§†é¢‘æ—¶é•¿ã€ç®€ä»‹ã€æ ‡é¢˜ç­‰
"""

import re
import json
import time
from typing import Dict, List, Optional, Tuple
from urllib.parse import urljoin, urlparse
from pathlib import Path


class EnhancedInfoExtractor:
    """å¢å¼ºçš„ä¿¡æ¯æå–å™¨"""
    
    def __init__(self, core=None):
        self.core = core
        self.cache_dir = Path("./cache/video_info")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ†è¾¨ç‡è´¨é‡æ˜ å°„
        self.quality_map = {
            '4K': (3840, 2160),
            '1080p': (1920, 1080),
            '720p': (1280, 720),
            '480p': (854, 480),
            '360p': (640, 360),
            '240p': (426, 240)
        }
        
        # åŸºç¡€URL
        self.base_url = "https://missav.ws"
    
    def extract_enhanced_video_info(self, url: str, use_cache: bool = True) -> Dict:
        """
        æå–å¢å¼ºçš„è§†é¢‘ä¿¡æ¯
        
        Args:
            url: è§†é¢‘URL
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            
        Returns:
            åŒ…å«è¯¦ç»†ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # ä¼˜å…ˆè¿›è¡Œå®æ—¶æŸ¥æ‰¾
            if not self.core:
                # å¦‚æœæ ¸å¿ƒæ¨¡å—æœªåˆå§‹åŒ–ï¼Œå°è¯•ä»ç¼“å­˜è·å–
                if use_cache:
                    cached_info = self._load_from_cache(url)
                    if cached_info:
                        cached_info['from_cache'] = True
                        cached_info['cache_reason'] = 'æ ¸å¿ƒæ¨¡å—æœªåˆå§‹åŒ–'
                        return cached_info
                return {"success": False, "error": "æ ¸å¿ƒæ¨¡å—æœªåˆå§‹åŒ–"}
            
            # å°è¯•è·å–é¡µé¢å†…å®¹
            content = self.core.fetch(url)
            if not content:
                # å¦‚æœæ— æ³•è·å–é¡µé¢å†…å®¹ï¼Œå°è¯•ä»ç¼“å­˜è·å–
                if use_cache:
                    cached_info = self._load_from_cache(url)
                    if cached_info:
                        cached_info['from_cache'] = True
                        cached_info['cache_reason'] = 'æ— æ³•è·å–é¡µé¢å†…å®¹'
                        return cached_info
                return {"success": False, "error": "æ— æ³•è·å–é¡µé¢å†…å®¹"}
            
            # å®æ—¶æå–ä¿¡æ¯
            try:
                # æå–åŸºç¡€ä¿¡æ¯
                basic_info = self._extract_basic_info(content, url)
                
                # æå–åˆ†è¾¨ç‡ä¿¡æ¯
                resolution_info = self._extract_resolution_info(content, url)
                
                # æå–è§†é¢‘æ—¶é•¿
                duration_info = self._extract_duration_info(content)
                
                # æå–è¯¦ç»†ä¿¡æ¯ï¼ˆæ¼”å‘˜ã€æ ‡ç­¾ã€ç³»åˆ—ç­‰ï¼‰
                detailed_info = self._extract_detailed_info(content, url)
                
                # æå–é¢„è§ˆè§†é¢‘ä¿¡æ¯
                preview_info = self._extract_preview_info(content, url)
                
                # æå–å°é¢ä¿¡æ¯
                cover_info = self._extract_cover_info(content, url)
                
                # åˆå¹¶æ‰€æœ‰ä¿¡æ¯
                enhanced_info = {
                    "success": True,
                    "url": url,
                    "extraction_time": time.time(),
                    "from_cache": False,  # æ˜ç¡®æ ‡æ³¨è¿™æ˜¯å®æ—¶è·å–çš„
                    **basic_info,
                    **resolution_info,
                    **duration_info,
                    **detailed_info,
                    **preview_info,
                    **cover_info
                }
                
                # ä¿å­˜åˆ°ç¼“å­˜
                if use_cache:
                    self._save_to_cache(url, enhanced_info)
                
                return enhanced_info
                
            except Exception as extraction_error:
                # å¦‚æœå®æ—¶æå–å¤±è´¥ï¼Œå°è¯•ä»ç¼“å­˜è·å–
                if use_cache:
                    cached_info = self._load_from_cache(url)
                    if cached_info:
                        cached_info['from_cache'] = True
                        cached_info['cache_reason'] = f'å®æ—¶æå–å¤±è´¥: {str(extraction_error)}'
                        return cached_info
                
                # å¦‚æœç¼“å­˜ä¹Ÿæ²¡æœ‰ï¼Œè¿”å›é”™è¯¯
                raise extraction_error
            
        except Exception as e:
            return {
                "success": False,
                "error": f"æå–è§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}",
                "url": url
            }
    
    def _extract_basic_info(self, content: str, url: str) -> Dict:
        """æå–åŸºç¡€ä¿¡æ¯"""
        info = {}
        
        try:
            # æå–æ ‡é¢˜
            title_patterns = [
                r'<h1[^>]*class="[^"]*text-base[^"]*"[^>]*>(.*?)</h1>',
                r'<title>(.*?)</title>',
                r'<h1[^>]*>(.*?)</h1>',
                r'og:title"[^>]*content="([^"]*)"'
            ]
            
            for pattern in title_patterns:
                match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
                if match:
                    title = match.group(1).strip()
                    # æ¸…ç†HTMLæ ‡ç­¾
                    title = re.sub(r'<[^>]+>', '', title)
                    if title and len(title) > 3:
                        info['title'] = title
                        break
            
            # æå–è§†é¢‘ä»£ç 
            code_patterns = [
                r'<span[^>]*class="[^"]*font-medium[^"]*"[^>]*>(.*?)</span>',
                r'è§†é¢‘ä»£ç [ï¼š:]\s*([A-Z0-9-]+)',
                r'ç•ªå·[ï¼š:]\s*([A-Z0-9-]+)',
                r'/([A-Z]{2,6}-\d{2,4})',
            ]
            
            for pattern in code_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    code = match.group(1).strip()
                    if re.match(r'^[A-Z0-9-]+$', code, re.IGNORECASE):
                        info['video_code'] = code.upper()
                        break
            
            # ä»URLæå–è§†é¢‘ä»£ç ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
            if 'video_code' not in info:
                url_code_match = re.search(r'/([A-Z]{2,6}-\d{2,4})', url, re.IGNORECASE)
                if url_code_match:
                    info['video_code'] = url_code_match.group(1).upper()
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_patterns = [
                r'class="[^"]*font-medium[^"]*"[^>]*>(\d{4}-\d{2}-\d{2})</time>',
                r'å‘å¸ƒæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
                r'ä¸Šæ˜ æ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
                r'(\d{4}-\d{2}-\d{2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, content)
                if match:
                    info['publish_date'] = match.group(1)
                    break
            
        except Exception as e:
            info['basic_info_error'] = str(e)
        
        return info
    
    def _extract_resolution_info(self, content: str, url: str) -> Dict:
        """æå–åˆ†è¾¨ç‡ä¿¡æ¯"""
        info = {}
        
        try:
            # æŸ¥æ‰¾M3U8æ’­æ”¾åˆ—è¡¨URL
            m3u8_patterns = [
                r"'m3u8(.*?)video",
                r'"m3u8_url":\s*"([^"]*)"',
                r'playlist\.m3u8[^"]*',
                r'master\.m3u8[^"]*'
            ]
            
            m3u8_url = None
            for pattern in m3u8_patterns:
                match = re.search(pattern, content)
                if match:
                    if pattern == r"'m3u8(.*?)video":
                        # ç‰¹æ®Šå¤„ç†MissAVçš„m3u8æ ¼å¼
                        url_parts = match.group(1).split("|")[::-1]
                        if len(url_parts) >= 8:
                            m3u8_url = f"{url_parts[1]}://{url_parts[2]}.{url_parts[3]}/{url_parts[4]}-{url_parts[5]}-{url_parts[6]}-{url_parts[7]}-{url_parts[8]}/playlist.m3u8"
                    else:
                        m3u8_url = match.group(1) if match.groups() else match.group(0)
                    break
            
            if m3u8_url:
                info['m3u8_url'] = m3u8_url
                
                # è·å–å¯ç”¨åˆ†è¾¨ç‡
                resolutions = self._get_available_resolutions(m3u8_url)
                if resolutions:
                    info['available_resolutions'] = resolutions
                    info['resolution_count'] = len(resolutions)
                    
                    # æ‰¾å‡ºæœ€é«˜å’Œæœ€ä½åˆ†è¾¨ç‡
                    if resolutions:
                        sorted_res = sorted(resolutions, key=lambda x: x.get('bandwidth', 0), reverse=True)
                        info['highest_resolution'] = sorted_res[0]
                        info['lowest_resolution'] = sorted_res[-1]
            
            # ä»é¡µé¢å†…å®¹ä¸­æŸ¥æ‰¾åˆ†è¾¨ç‡ä¿¡æ¯
            resolution_patterns = [
                r'(\d{3,4})[xXÃ—](\d{3,4})',
                r'(\d{3,4}p)',
                r'(4K|HD|FHD|UHD)'
            ]
            
            found_resolutions = []
            for pattern in resolution_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple):
                        if len(match) == 2 and match[0].isdigit() and match[1].isdigit():
                            found_resolutions.append(f"{match[0]}x{match[1]}")
                    else:
                        found_resolutions.append(match)
            
            if found_resolutions:
                info['page_resolutions'] = list(set(found_resolutions))
            
        except Exception as e:
            info['resolution_info_error'] = str(e)
        
        return info
    
    def _get_available_resolutions(self, m3u8_url: str) -> List[Dict]:
        """è·å–M3U8æ’­æ”¾åˆ—è¡¨ä¸­çš„å¯ç”¨åˆ†è¾¨ç‡"""
        try:
            if not self.core:
                return []
            
            # è·å–ä¸»æ’­æ”¾åˆ—è¡¨
            master_content = self.core.fetch(m3u8_url)
            if not master_content:
                return []
            
            resolutions = []
            
            # è§£æEXT-X-STREAM-INFæ ‡ç­¾
            stream_pattern = r'#EXT-X-STREAM-INF:([^\n]+)\n([^\n]+)'
            matches = re.findall(stream_pattern, master_content)
            
            for info_line, url_line in matches:
                resolution_info = {}
                
                # æå–åˆ†è¾¨ç‡
                resolution_match = re.search(r'RESOLUTION=(\d+)x(\d+)', info_line)
                if resolution_match:
                    width, height = resolution_match.groups()
                    resolution_info['width'] = int(width)
                    resolution_info['height'] = int(height)
                    resolution_info['resolution'] = f"{width}x{height}"
                    
                    # åˆ¤æ–­è´¨é‡ç­‰çº§
                    for quality, (q_width, q_height) in self.quality_map.items():
                        if int(width) == q_width and int(height) == q_height:
                            resolution_info['quality'] = quality
                            break
                    else:
                        if int(height) >= 1080:
                            resolution_info['quality'] = 'HD+'
                        elif int(height) >= 720:
                            resolution_info['quality'] = 'HD'
                        else:
                            resolution_info['quality'] = 'SD'
                
                # æå–å¸¦å®½ï¼ˆä»…ç”¨äºå†…éƒ¨æ’åºï¼Œä¸æ˜¾ç¤ºç»™ç”¨æˆ·ï¼‰
                bandwidth_match = re.search(r'BANDWIDTH=(\d+)', info_line)
                if bandwidth_match:
                    resolution_info['bandwidth'] = int(bandwidth_match.group(1))
                    # ä¸å†æ˜¾ç¤ºå¸¦å®½ä¿¡æ¯ç»™ç”¨æˆ·
                
                # æå–ç¼–ç ä¿¡æ¯
                codecs_match = re.search(r'CODECS="([^"]*)"', info_line)
                if codecs_match:
                    resolution_info['codecs'] = codecs_match.group(1)
                
                # æå–å¸§ç‡
                frame_rate_match = re.search(r'FRAME-RATE=([\d.]+)', info_line)
                if frame_rate_match:
                    resolution_info['frame_rate'] = float(frame_rate_match.group(1))
                
                resolution_info['url'] = url_line.strip()
                resolutions.append(resolution_info)
            
            return resolutions
            
        except Exception as e:
            return []
    
    def _extract_duration_info(self, content: str) -> Dict:
        """æå–è§†é¢‘æ—¶é•¿ä¿¡æ¯"""
        info = {}
        
        try:
            # ä¼˜å…ˆä»og:video:duration metaæ ‡ç­¾æå–ï¼ˆæœ€å¯é ï¼‰
            og_duration_match = re.search(r'<meta[^>]*property="og:video:duration"[^>]*content="(\d+)"', content, re.IGNORECASE)
            if og_duration_match:
                total_seconds = int(og_duration_match.group(1))
                if total_seconds > 0:
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    seconds = total_seconds % 60
                    
                    info['duration_seconds'] = total_seconds
                    if hours > 0:
                        info['duration'] = f"{hours}:{minutes:02d}:{seconds:02d}"
                    else:
                        info['duration'] = f"{minutes}:{seconds:02d}"
                    
                    # äººæ€§åŒ–æè¿°
                    if hours > 0:
                        info['duration_human'] = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"
                    else:
                        info['duration_human'] = f"{minutes}åˆ†é’Ÿ"
                    
                    # åˆ†ç±»
                    if total_seconds >= 3600:
                        info['duration_category'] = 'long'  # é•¿ç‰‡ï¼ˆ1å°æ—¶ä»¥ä¸Šï¼‰
                    elif total_seconds >= 1800:
                        info['duration_category'] = 'medium'  # ä¸­ç­‰ï¼ˆ30åˆ†é’Ÿ-1å°æ—¶ï¼‰
                    else:
                        info['duration_category'] = 'short'  # çŸ­ç‰‡ï¼ˆ30åˆ†é’Ÿä»¥ä¸‹ï¼‰
                    
                    return info
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°og:video:durationï¼Œå°è¯•å…¶ä»–æ¨¡å¼
            duration_patterns = [
                # JSONä¸­çš„durationå­—æ®µï¼ˆç§’æ•°ï¼‰
                (r'"duration":\s*"?(\d+)"?', "JSON duration"),
                (r'duration["\']?\s*:\s*["\']?(\d+)["\']?', "JS duration"),
                
                # å…¶ä»–metaæ ‡ç­¾
                (r'<meta[^>]*name="duration"[^>]*content="(\d+)"', "meta duration (ç§’æ•°)"),
                (r'<meta[^>]*name="video:duration"[^>]*content="(\d+)"', "meta video:duration"),
                
                # é¡µé¢ä¸­çš„æ—¶é•¿æ˜¾ç¤º
                (r'æ™‚é•·[ï¼š:]\s*(\d{1,3}:\d{2})', "æ—¥æ–‡æ™‚é•·"),
                (r'æ—¶é•¿[ï¼š:]\s*(\d{1,3}:\d{2})', "ä¸­æ–‡æ—¶é•¿"),
                (r'Duration[ï¼š:]\s*(\d{1,3}:\d{2})', "è‹±æ–‡Duration"),
                (r'é•·åº¦[ï¼š:]\s*(\d{1,3}:\d{2})', "ç¹ä½“é•·åº¦"),
                
                # å¸¦å°æ—¶çš„æ ¼å¼
                (r'æ™‚é•·[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})', "æ—¥æ–‡æ™‚é•·(å¸¦å°æ—¶)"),
                (r'æ—¶é•¿[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})', "ä¸­æ–‡æ—¶é•¿(å¸¦å°æ—¶)"),
                (r'Duration[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})', "è‹±æ–‡Duration(å¸¦å°æ—¶)"),
                
                # ä»è§†é¢‘ä¿¡æ¯åŒºåŸŸæå–
                (r'<div[^>]*class="[^"]*duration[^"]*"[^>]*>.*?(\d{1,3}:\d{2}).*?</div>', "duration classåŒºåŸŸ"),
                (r'<span[^>]*class="[^"]*time[^"]*"[^>]*>(\d{1,3}:\d{2})</span>', "time classåŒºåŸŸ"),
                
                # ä»metaæ ‡ç­¾æå–æ—¶é—´æ ¼å¼
                (r'<meta[^>]*name="duration"[^>]*content="(\d{1,3}:\d{2})"', "meta duration (æ—¶é—´)"),
                
                # ä»scriptæ ‡ç­¾ä¸­çš„å˜é‡æå–
                (r'var\s+duration\s*=\s*["\'](\d{1,3}:\d{2})["\']', "JSå˜é‡duration"),
                (r'duration\s*=\s*["\'](\d{1,3}:\d{2})["\']', "JSèµ‹å€¼duration"),
            ]
            
            # å°è¯•å…¶ä»–æ¨¡å¼
            duration_found = False
            
            for pattern, description in duration_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    duration_str = match.strip()
                    
                    # è·³è¿‡æ˜æ˜¾é”™è¯¯çš„æ—¶é•¿
                    if not duration_str or duration_str in ['0', '00:00', '0:00']:
                        continue
                    
                    # è§£ææ—¶é•¿
                    if duration_str.isdigit():
                        # çº¯æ•°å­—ï¼Œè®¤ä¸ºæ˜¯ç§’æ•°
                        total_seconds = int(duration_str)
                        if total_seconds > 0:  # ç¡®ä¿ä¸æ˜¯0
                            hours = total_seconds // 3600
                            minutes = (total_seconds % 3600) // 60
                            seconds = total_seconds % 60
                            
                            info['duration_seconds'] = total_seconds
                            if hours > 0:
                                info['duration'] = f"{hours}:{minutes:02d}:{seconds:02d}"
                            else:
                                info['duration'] = f"{minutes}:{seconds:02d}"
                            
                            # äººæ€§åŒ–æè¿°
                            if hours > 0:
                                info['duration_human'] = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"
                            else:
                                info['duration_human'] = f"{minutes}åˆ†é’Ÿ"
                            
                            # åˆ†ç±»
                            if total_seconds >= 3600:
                                info['duration_category'] = 'long'
                            elif total_seconds >= 1800:
                                info['duration_category'] = 'medium'
                            else:
                                info['duration_category'] = 'short'
                            
                            duration_found = True
                            break
                    elif ':' in duration_str:
                        # æ—¶é—´æ ¼å¼
                        try:
                            time_parts = duration_str.split(':')
                            if len(time_parts) == 2:
                                # MM:SS æ ¼å¼
                                minutes, seconds = map(int, time_parts)
                                if minutes > 0 or seconds > 0:  # ç¡®ä¿ä¸æ˜¯00:00
                                    total_seconds = minutes * 60 + seconds
                                    info['duration_seconds'] = total_seconds
                                    info['duration'] = f"{minutes}:{seconds:02d}"
                                    info['duration_human'] = f"{minutes}åˆ†é’Ÿ"
                                    info['duration_category'] = 'short' if total_seconds < 1800 else 'medium'
                                    duration_found = True
                                    break
                            elif len(time_parts) == 3:
                                # HH:MM:SS æ ¼å¼
                                hours, minutes, seconds = map(int, time_parts)
                                if hours > 0 or minutes > 0 or seconds > 0:  # ç¡®ä¿ä¸æ˜¯00:00:00
                                    total_seconds = hours * 3600 + minutes * 60 + seconds
                                    info['duration_seconds'] = total_seconds
                                    info['duration'] = f"{hours}:{minutes:02d}:{seconds:02d}"
                                    if hours > 0:
                                        info['duration_human'] = f"{hours}å°æ—¶{minutes}åˆ†é’Ÿ"
                                    else:
                                        info['duration_human'] = f"{minutes}åˆ†é’Ÿ"
                                    info['duration_category'] = 'long' if total_seconds >= 3600 else ('medium' if total_seconds >= 1800 else 'short')
                                    duration_found = True
                                    break
                        except ValueError:
                            continue
                
                if duration_found:
                    break
            
        except Exception as e:
            info['duration_info_error'] = str(e)
        
        return info
    
    def _extract_detailed_info(self, content: str, url: str) -> Dict:
        """æå–è¯¦ç»†ä¿¡æ¯ï¼šæ¼”å‘˜ã€æ ‡ç­¾ã€ç³»åˆ—ã€å‘è¡Œå•†ç­‰"""
        info = {}
        
        try:
            # æå–ç®€ä»‹/æè¿°
            description = self._extract_description(content)
            if description:
                info['description'] = description
                info['description_length'] = len(description)
                # ç®€ä»‹æ‘˜è¦ï¼ˆå‰200å­—ç¬¦ï¼‰
                if len(description) > 200:
                    info['description_summary'] = description[:200] + "..."
                else:
                    info['description_summary'] = description
            
            # æå–å‘è¡Œæ—¥æœŸï¼ˆæ›´ç²¾ç¡®ï¼‰
            release_date = self._extract_release_date(content)
            if release_date:
                info['release_date'] = release_date
            
            # æå–ç•ªå·ï¼ˆè§†é¢‘ä»£ç çš„å¦ä¸€ç§è¡¨è¾¾ï¼‰
            video_code = self._extract_video_code(content, url)
            if video_code:
                info['video_code'] = video_code
                info['ç•ªè™Ÿ'] = video_code  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
            # æå–æ¼”å‘˜ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
            actresses_info = self._extract_actresses_with_links(content)
            if actresses_info:
                info['actresses'] = [actress['name'] for actress in actresses_info]
                info['actresses_with_links'] = actresses_info
                info['actress_count'] = len(actresses_info)
                info['å¥³å„ª'] = actresses_info  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
            # æå–ç±»å‹/æ ‡ç­¾ï¼ˆå¸¦é“¾æ¥ï¼‰
            types_info = self._extract_types_with_links(content)
            if types_info:
                info['types'] = [type_item['name'] for type_item in types_info]
                info['types_with_links'] = types_info
                info['type_count'] = len(types_info)
                info['é¡å‹'] = types_info  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
            # æå–ç³»åˆ—ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
            series_info = self._extract_series_with_links(content)
            if series_info:
                info['series'] = series_info['name']
                info['series_with_link'] = series_info
                info['ç³»åˆ—'] = series_info  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
            # æå–å‘è¡Œå•†ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
            publisher_info = self._extract_publisher_with_links(content)
            if publisher_info:
                info['publisher'] = publisher_info['name']
                info['publisher_with_link'] = publisher_info
                info['ç™¼è¡Œå•†'] = publisher_info  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
            # æå–æ ‡ç­¾ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
            tags_info = self._extract_tags_with_links(content)
            if tags_info:
                info['tags'] = [tag['name'] for tag in tags_info]
                info['tags_with_links'] = tags_info
                info['tag_count'] = len(tags_info)
                info['æ¨™ç±¤'] = tags_info  # æ·»åŠ ä¸­æ–‡å­—æ®µ
            
        except Exception as e:
            info['detailed_info_error'] = str(e)
        
        return info
    
    def _extract_description(self, content: str) -> str:
        """æå–è§†é¢‘æè¿°/ç®€ä»‹"""
        description_patterns = [
            # Metaæ ‡ç­¾ä¸­çš„æè¿°
            r'<meta[^>]*name="description"[^>]*content="([^"]*)"',
            r'<meta[^>]*property="og:description"[^>]*content="([^"]*)"',
            
            # é¡µé¢ä¸­çš„æè¿°åŒºåŸŸ
            r'<div[^>]*class="[^"]*description[^"]*"[^>]*>(.*?)</div>',
            r'<p[^>]*class="[^"]*description[^"]*"[^>]*>(.*?)</p>',
            r'<div[^>]*class="[^"]*summary[^"]*"[^>]*>(.*?)</div>',
            
            # ä¸­æ–‡æ ‡ç­¾
            r'ç®€ä»‹[ï¼š:]\s*([^<\n]+)',
            r'ä»‹ç»[ï¼š:]\s*([^<\n]+)',
            r'å…§å®¹[ï¼š:]\s*([^<\n]+)',
            
            # è‹±æ–‡æ ‡ç­¾
            r'Description[ï¼š:]\s*([^<\n]+)',
            r'Summary[ï¼š:]\s*([^<\n]+)',
        ]
        
        for pattern in description_patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                description = match.group(1).strip()
                # æ¸…ç†HTMLæ ‡ç­¾
                description = re.sub(r'<[^>]+>', '', description)
                # æ¸…ç†å¤šä½™ç©ºç™½
                description = ' '.join(description.split())
                # è§£ç HTMLå®ä½“
                description = description.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
                
                if description and len(description) > 10:
                    return description
        
        return ""
    
    def _extract_release_date(self, content: str) -> str:
        """æå–å‘è¡Œæ—¥æœŸ"""
        date_patterns = [
            # å‘è¡Œæ—¥æœŸç›¸å…³
            r'ç™¼è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
            r'å‘è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
            r'Release Date[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
            r'ä¸Šæ˜ æ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
            
            # ä»timeæ ‡ç­¾æå–
            r'<time[^>]*datetime="(\d{4}-\d{2}-\d{2})"',
            r'<time[^>]*>(\d{4}-\d{2}-\d{2})</time>',
            
            # ä»metaæ ‡ç­¾æå–
            r'<meta[^>]*name="release_date"[^>]*content="(\d{4}-\d{2}-\d{2})"',
            
            # é€šç”¨æ—¥æœŸæ ¼å¼
            r'(\d{4}-\d{2}-\d{2})',
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return ""
    
    def _extract_video_code(self, content: str, url: str) -> str:
        """æå–è§†é¢‘ä»£ç /ç•ªå·"""
        code_patterns = [
            # ä»é¡µé¢å†…å®¹æå–
            r'ç•ªè™Ÿ[ï¼š:]\s*([A-Z0-9-]+)',
            r'å“ç•ª[ï¼š:]\s*([A-Z0-9-]+)',
            r'Code[ï¼š:]\s*([A-Z0-9-]+)',
            r'<span[^>]*class="[^"]*code[^"]*"[^>]*>([A-Z0-9-]+)</span>',
            
            # ä»URLæå–
            r'/([A-Z]{2,6}-\d{2,4})',
            r'/dm\d+/([a-zA-Z]+-\d+)',
        ]
        
        for pattern in code_patterns:
            if pattern.startswith('/'):
                # URLæ¨¡å¼
                match = re.search(pattern, url, re.IGNORECASE)
            else:
                # å†…å®¹æ¨¡å¼
                match = re.search(pattern, content, re.IGNORECASE)
            
            if match:
                code = match.group(1).strip().upper()
                if re.match(r'^[A-Z0-9-]+$', code):
                    return code
        
        return ""
    
    def _extract_actresses_with_links(self, content: str) -> List[Dict]:
        """æå–æ¼”å‘˜ä¿¡æ¯ï¼ˆåŒ…å«é“¾æ¥ï¼‰"""
        actresses = []
        
        # æ¼”å‘˜é“¾æ¥æ¨¡å¼
        actress_patterns = [
            r'<a[^>]*href="([^"]*(?:actress|å¥³å„ª|performer)[^"]*)"[^>]*>([^<]+)</a>',
            r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*actress[^"]*"[^>]*>([^<]+)</a>',
            r'href="(/[^"]*actress[^"]*)"[^>]*>([^<]+)</a>',
        ]
        
        for pattern in actress_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                link, name = match
                name = name.strip()
                if name and len(name) > 1:
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    if link.startswith('/'):
                        link = self.base_url + link
                    elif not link.startswith('http'):
                        link = self.base_url + '/' + link
                    
                    actress_info = {
                        'name': name,
                        'link': link
                    }
                    
                    # é¿å…é‡å¤
                    if not any(a['name'] == name for a in actresses):
                        actresses.append(actress_info)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•æå–çº¯æ–‡æœ¬æ¼”å‘˜å
        if not actresses:
            text_patterns = [
                r'å¥³å„ª[ï¼š:]\s*([^<\n]+)',
                r'æ¼”å‘˜[ï¼š:]\s*([^<\n]+)',
                r'Actress[ï¼š:]\s*([^<\n]+)',
                r'å‡ºæ¼”[ï¼š:]\s*([^<\n]+)',
            ]
            
            for pattern in text_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    names_text = match.group(1).strip()
                    # åˆ†å‰²å¤šä¸ªæ¼”å‘˜å
                    names = re.split(r'[,ï¼Œã€]', names_text)
                    for name in names:
                        name = name.strip()
                        if name and len(name) > 1:
                            actresses.append({
                                'name': name,
                                'link': ''
                            })
                    break
        
        return actresses
    
    def _extract_types_with_links(self, content: str) -> List[Dict]:
        """æå–ç±»å‹/æ ‡ç­¾ä¿¡æ¯ï¼ˆåŒ…å«é“¾æ¥ï¼‰"""
        types = []
        
        # ç±»å‹é“¾æ¥æ¨¡å¼
        type_patterns = [
            r'<a[^>]*href="([^"]*(?:genre|tag|category|é¡å‹)[^"]*)"[^>]*>([^<]+)</a>',
            r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*(?:genre|tag|category)[^"]*"[^>]*>([^<]+)</a>',
            r'href="(/[^"]*(?:genre|tag)[^"]*)"[^>]*>([^<]+)</a>',
        ]
        
        for pattern in type_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                link, name = match
                name = name.strip()
                if name and len(name) > 1:
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    if link.startswith('/'):
                        link = self.base_url + link
                    elif not link.startswith('http'):
                        link = self.base_url + '/' + link
                    
                    type_info = {
                        'name': name,
                        'link': link
                    }
                    
                    # é¿å…é‡å¤
                    if not any(t['name'] == name for t in types):
                        types.append(type_info)
        
        return types
    
    def _extract_series_with_links(self, content: str) -> Dict:
        """æå–ç³»åˆ—ä¿¡æ¯ï¼ˆåŒ…å«é“¾æ¥ï¼‰"""
        series_patterns = [
            r'<a[^>]*href="([^"]*(?:series|ç³»åˆ—)[^"]*)"[^>]*>([^<]+)</a>',
            r'ç³»åˆ—[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
            r'Series[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
        ]
        
        for pattern in series_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                link, name = match.groups()
                name = name.strip()
                if name and len(name) > 1:
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    if link.startswith('/'):
                        link = self.base_url + link
                    elif not link.startswith('http'):
                        link = self.base_url + '/' + link
                    
                    return {
                        'name': name,
                        'link': link
                    }
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•æå–çº¯æ–‡æœ¬
        text_patterns = [
            r'ç³»åˆ—[ï¼š:]\s*([^<\n]+)',
            r'Series[ï¼š:]\s*([^<\n]+)',
        ]
        
        for pattern in text_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                name = match.group(1).strip()
                if name and len(name) > 1:
                    return {
                        'name': name,
                        'link': ''
                    }
        
        return {}
    
    def _extract_publisher_with_links(self, content: str) -> Dict:
        """æå–å‘è¡Œå•†ä¿¡æ¯ï¼ˆåŒ…å«é“¾æ¥ï¼‰"""
        publisher_patterns = [
            r'<a[^>]*href="([^"]*(?:studio|publisher|maker|ç™¼è¡Œå•†)[^"]*)"[^>]*>([^<]+)</a>',
            r'ç™¼è¡Œå•†[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
            r'Studio[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
            r'Maker[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
        ]
        
        for pattern in publisher_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                link, name = match.groups()
                name = name.strip()
                if name and len(name) > 1:
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    if link.startswith('/'):
                        link = self.base_url + link
                    elif not link.startswith('http'):
                        link = self.base_url + '/' + link
                    
                    return {
                        'name': name,
                        'link': link
                    }
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é“¾æ¥ï¼Œå°è¯•æå–çº¯æ–‡æœ¬
        text_patterns = [
            r'ç™¼è¡Œå•†[ï¼š:]\s*([^<\n]+)',
            r'å‘è¡Œå•†[ï¼š:]\s*([^<\n]+)',
            r'Studio[ï¼š:]\s*([^<\n]+)',
            r'Maker[ï¼š:]\s*([^<\n]+)',
        ]
        
        for pattern in text_patterns:
            match = re.search(pattern, content, re.IGNORECASE)
            if match:
                name = match.group(1).strip()
                if name and len(name) > 1:
                    return {
                        'name': name,
                        'link': ''
                    }
        
        return {}
    
    def _extract_tags_with_links(self, content: str) -> List[Dict]:
        """æå–æ ‡ç­¾ä¿¡æ¯ï¼ˆåŒ…å«é“¾æ¥ï¼‰"""
        tags = []
        
        # æ ‡ç­¾é“¾æ¥æ¨¡å¼
        tag_patterns = [
            r'<a[^>]*href="([^"]*(?:tag|label|æ¨™ç±¤)[^"]*)"[^>]*>([^<]+)</a>',
            r'æ¨™ç±¤[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
            r'Tags[ï¼š:]\s*<a[^>]*href="([^"]*)"[^>]*>([^<]+)</a>',
        ]
        
        for pattern in tag_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                link, name = match
                name = name.strip()
                if name and len(name) > 1:
                    # æ„å»ºå®Œæ•´é“¾æ¥
                    if link.startswith('/'):
                        link = self.base_url + link
                    elif not link.startswith('http'):
                        link = self.base_url + '/' + link
                    
                    tag_info = {
                        'name': name,
                        'link': link
                    }
                    
                    # é¿å…é‡å¤
                    if not any(t['name'] == name for t in tags):
                        tags.append(tag_info)
        
        return tags
    
    def _extract_preview_info(self, content: str, url: str) -> Dict:
        """æå–é¢„è§ˆè§†é¢‘ä¿¡æ¯"""
        info = {}
        
        try:
            # ä»URLæå–DVD ID
            dvd_id = self._extract_dvd_id_from_url(url)
            
            if dvd_id:
                # åŸºäºå‘ç°çš„cdnUrlå‡½æ•°æ„é€ é¢„è§ˆè§†é¢‘URL
                # cdnUrl(path) { return `https://fourhoi.com${path}` }
                # é¢„è§ˆè§†é¢‘æ¨¡å¼: cdnUrl(`/${item.dvd_id}/preview.mp4`)
                preview_url = f"https://fourhoi.com/{dvd_id}/preview.mp4"
                
                # éªŒè¯é¢„è§ˆè§†é¢‘URLæ˜¯å¦å¯è®¿é—®
                if self._verify_preview_url(preview_url):
                    info['preview_videos'] = [preview_url]
                    info['preview_count'] = 1
                    info['has_preview'] = True
                    info['main_preview'] = preview_url
                else:
                    info['has_preview'] = False
                    info['preview_count'] = 0
            else:
                info['has_preview'] = False
                info['preview_count'] = 0
                info['preview_extraction_error'] = 'Unable to extract DVD ID from URL'
            
            # æŸ¥æ‰¾é¢„è§ˆå›¾ç‰‡ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
            preview_image_patterns = [
                r'data-src="([^"]*preview[^"]*)"',
                r'src="([^"]*preview[^"]*)"',
                r'"preview_image":\s*"([^"]*)"',
            ]
            
            preview_images = []
            for pattern in preview_image_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match and match not in preview_images:
                        if match.startswith('http'):
                            preview_images.append(match)
                        elif match.startswith('/'):
                            base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
                            preview_images.append(urljoin(base_url, match))
            
            if preview_images:
                info['preview_images'] = preview_images
                info['preview_image_count'] = len(preview_images)
            
        except Exception as e:
            info['preview_info_error'] = str(e)
            info['has_preview'] = False
            info['preview_count'] = 0
        
        return info
    
    def _extract_dvd_id_from_url(self, url: str) -> Optional[str]:
        """ä»URLæå–DVD ID"""
        try:
            # ä»URLè·¯å¾„ä¸­æå–æœ€åä¸€éƒ¨åˆ†ä½œä¸ºDVD ID
            # ä¾‹å¦‚: https://missav.ws/dm44/jul-875 -> jul-875
            path_parts = url.rstrip('/').split('/')
            if path_parts:
                dvd_id = path_parts[-1]
                # éªŒè¯DVD IDæ ¼å¼ï¼ˆé€šå¸¸åŒ…å«å­—æ¯å’Œæ•°å­—ï¼Œå¯èƒ½æœ‰è¿å­—ç¬¦ï¼‰
                if re.match(r'^[a-zA-Z0-9\-]+$', dvd_id) and len(dvd_id) > 2:
                    return dvd_id
        except Exception:
            pass
        return None
    
    def _verify_preview_url(self, preview_url: str) -> bool:
        """éªŒè¯é¢„è§ˆè§†é¢‘URLæ˜¯å¦å¯è®¿é—®"""
        try:
            import requests
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': 'https://missav.ws/',
                'Accept': 'video/mp4,video/*,*/*;q=0.9',
            }
            
            response = requests.head(preview_url, headers=headers, timeout=10)
            return response.status_code == 200
            
        except Exception:
            return False
    
    def _extract_cover_info(self, content: str, url: str) -> Dict:
        """æå–å°é¢ä¿¡æ¯"""
        info = {}
        
        try:
            # æŸ¥æ‰¾å°é¢å›¾ç‰‡
            cover_patterns = [
                r'og:image"[^>]*content="([^"]*)"',
                r'"thumbnail":\s*"([^"]*)"',
                r'<img[^>]*class="[^"]*cover[^"]*"[^>]*src="([^"]*)"',
                r'cover-n\.jpg',
                r'poster[^"]*"([^"]*)"',
            ]
            
            cover_urls = []
            for pattern in cover_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                for match in matches:
                    if match:
                        # æ„å»ºå®Œæ•´URL
                        if match.startswith('http'):
                            cover_urls.append(match)
                        elif match.startswith('/'):
                            base_url = f"{urlparse(url).scheme}://{urlparse(url).netloc}"
                            cover_urls.append(urljoin(base_url, match))
            
            if cover_urls:
                info['cover_images'] = list(set(cover_urls))
                info['main_cover'] = cover_urls[0]
                info['cover_count'] = len(cover_urls)
            
            # æŸ¥æ‰¾é«˜æ¸…å°é¢
            hd_cover_patterns = [
                r'cover-n\.jpg',
                r'cover-hd\.jpg',
                r'poster-hd\.jpg',
            ]
            
            for pattern in hd_cover_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    info['has_hd_cover'] = True
                    break
            else:
                info['has_hd_cover'] = False
            
        except Exception as e:
            info['cover_info_error'] = str(e)
        
        return info
    
    def _load_from_cache(self, url: str) -> Optional[Dict]:
        """ä»ç¼“å­˜åŠ è½½ä¿¡æ¯"""
        try:
            # ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
            import hashlib
            url_hash = hashlib.md5(url.encode()).hexdigest()
            cache_file = self.cache_dir / f"{url_hash}.json"
            
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
                if time.time() - cached_data.get('extraction_time', 0) < 86400:
                    cached_data['from_cache'] = True
                    return cached_data
            
        except Exception:
            pass
        
        return None
    
    def _save_to_cache(self, url: str, info: Dict) -> None:
        """ä¿å­˜ä¿¡æ¯åˆ°ç¼“å­˜"""
        try:
            import hashlib
            url_hash = hashlib.md5(url.encode()).hexdigest()
            cache_file = self.cache_dir / f"{url_hash}.json"
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(info, f, ensure_ascii=False, indent=2)
                
        except Exception:
            pass
    
    def format_info_response(self, info: Dict) -> str:
        """æ ¼å¼åŒ–ä¿¡æ¯å“åº”ä¸ºæ–‡æœ¬"""
        if not info.get("success"):
            return f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {info.get('error', 'æœªçŸ¥é”™è¯¯')}"
        
        response_text = "### MissAV å¢å¼ºè§†é¢‘ä¿¡æ¯ ###\n\n"
        
        # åŸºç¡€ä¿¡æ¯
        if info.get('title'):
            response_text += f"**æ¨™é¡Œ**: {info['title']}\n"
        
        if info.get('video_code'):
            response_text += f"**ç•ªè™Ÿ**: {info['video_code']}\n"
        
        if info.get('release_date'):
            response_text += f"**ç™¼è¡Œæ—¥æœŸ**: {info['release_date']}\n"
        elif info.get('publish_date'):
            response_text += f"**ç™¼è¡Œæ—¥æœŸ**: {info['publish_date']}\n"
        
        # æ—¶é•¿ä¿¡æ¯
        if info.get('duration') and info.get('duration') != '00:00:00':
            response_text += f"**æ™‚é•·**: {info['duration']}"
            if info.get('duration_human'):
                response_text += f" ({info['duration_human']})"
            response_text += "\n"
        
        # æ¼”å‘˜ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
        if info.get('actresses_with_links'):
            response_text += f"\n**å¥³å„ª**: "
            actress_list = []
            for actress in info['actresses_with_links']:
                if actress.get('link'):
                    actress_list.append(f"[{actress['name']}]({actress['link']})")
                else:
                    actress_list.append(actress['name'])
            response_text += ', '.join(actress_list) + "\n"
        elif info.get('actresses'):
            response_text += f"\n**å¥³å„ª**: {', '.join(info['actresses'])}\n"
        
        # ç±»å‹ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
        if info.get('types_with_links'):
            response_text += f"**é¡å‹**: "
            type_list = []
            for type_item in info['types_with_links']:
                if type_item.get('link'):
                    type_list.append(f"[{type_item['name']}]({type_item['link']})")
                else:
                    type_list.append(type_item['name'])
            response_text += ', '.join(type_list) + "\n"
        elif info.get('types'):
            response_text += f"**é¡å‹**: {', '.join(info['types'])}\n"
        
        # ç³»åˆ—ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
        if info.get('series_with_link'):
            series = info['series_with_link']
            if series.get('link'):
                response_text += f"**ç³»åˆ—**: [{series['name']}]({series['link']})\n"
            else:
                response_text += f"**ç³»åˆ—**: {series['name']}\n"
        elif info.get('series'):
            response_text += f"**ç³»åˆ—**: {info['series']}\n"
        
        # å‘è¡Œå•†ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
        if info.get('publisher_with_link'):
            publisher = info['publisher_with_link']
            if publisher.get('link'):
                response_text += f"**ç™¼è¡Œå•†**: [{publisher['name']}]({publisher['link']})\n"
            else:
                response_text += f"**ç™¼è¡Œå•†**: {publisher['name']}\n"
        elif info.get('publisher'):
            response_text += f"**ç™¼è¡Œå•†**: {info['publisher']}\n"
        
        # æ ‡ç­¾ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰
        if info.get('tags_with_links'):
            response_text += f"**æ¨™ç±¤**: "
            tag_list = []
            for tag in info['tags_with_links']:
                if tag.get('link'):
                    tag_list.append(f"[{tag['name']}]({tag['link']})")
                else:
                    tag_list.append(tag['name'])
            response_text += ', '.join(tag_list) + "\n"
        elif info.get('tags'):
            response_text += f"**æ¨™ç±¤**: {', '.join(info['tags'])}\n"
        
        # åˆ†è¾¨ç‡ä¿¡æ¯ï¼ˆä¸æ˜¾ç¤ºå¸¦å®½ï¼‰
        if info.get('available_resolutions'):
            response_text += f"\n**å¯ç”¨åˆ†è¾¨ç‡** ({info.get('resolution_count', 0)}ä¸ª):\n"
            for i, res in enumerate(info['available_resolutions'][:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                quality = res.get('quality', 'æœªçŸ¥')
                resolution = res.get('resolution', 'æœªçŸ¥')
                response_text += f"  {i}. {quality} ({resolution})\n"
            
            if len(info['available_resolutions']) > 5:
                response_text += f"  ... è¿˜æœ‰ {len(info['available_resolutions']) - 5} ä¸ªåˆ†è¾¨ç‡\n"
        
        # ç®€ä»‹ä¿¡æ¯
        if info.get('description_summary'):
            response_text += f"\n**ç°¡ä»‹**: {info['description_summary']}\n"
        
        # å°é¢å’Œé¢„è§ˆ
        if info.get('main_cover'):
            response_text += f"\n**å°é¢åœ–ç‰‡**: {info['main_cover']}\n"
        
        if info.get('has_preview'):
            response_text += f"**é è¦½è¦–é »**: å¯ç”¨ ({info.get('preview_count', 0)}ä¸ª)\n"
            if info.get('preview_videos'):
                response_text += f"  ä¸»é è¦½: {info['preview_videos'][0]}\n"
        
        # æŠ€æœ¯ä¿¡æ¯
        if info.get('m3u8_url'):
            response_text += f"\n**M3U8æ’­æ”¾åˆ—è¡¨**: {info['m3u8_url']}\n"
        
        response_text += f"\n**åŸå§‹URL**: {info['url']}\n"
        
        # ç¼“å­˜ä¿¡æ¯
        if info.get('from_cache'):
            cache_reason = info.get('cache_reason', 'ä½¿ç”¨ç¼“å­˜æ•°æ®')
            response_text += f"\nğŸ’¾ **ä¿¡æ¯æ¥æº**: ç¼“å­˜æ•°æ® ({cache_reason})\n"
        else:
            response_text += f"\nğŸ”„ **ä¿¡æ¯æ¥æº**: å®æ—¶è·å–\n"
        
        return response_text


def test_enhanced_info_extractor():
    """æµ‹è¯•å¢å¼ºä¿¡æ¯æå–å™¨"""
    print("ğŸ” æµ‹è¯•å¢å¼ºä¿¡æ¯æå–å™¨")
    print("=" * 50)
    
    # è¿™é‡Œéœ€è¦ä¸€ä¸ªå®é™…çš„coreå®ä¾‹æ¥æµ‹è¯•
    # extractor = EnhancedInfoExtractor(core=some_core)
    # result = extractor.extract_enhanced_video_info("https://missav.ws/some-video")
    # print(extractor.format_info_response(result))
    
    print("âœ… å¢å¼ºä¿¡æ¯æå–å™¨æ¨¡å—å·²åˆ›å»º")


if __name__ == "__main__":
    test_enhanced_info_extractor()