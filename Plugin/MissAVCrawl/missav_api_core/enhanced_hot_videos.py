#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MissAV çœŸå®çƒ­æ¦œçˆ¬è™«æ¨¡å—
ä»æŒ‡å®šçš„çƒ­æ¦œé¡µé¢çˆ¬å–çœŸå®æ•°æ®ï¼Œç»“åˆæ’åºè¿‡æ»¤å™¨åŠŸèƒ½
"""

from typing import Dict, List, Optional
from .sort_filter_module import SortFilterModule
# ç§»é™¤è™šæ„æ•°æ®å¤‡ç”¨æºå¯¼å…¥
import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import time
import sys
import random
from datetime import datetime

from .debug_utils import debug_print


class RealMissAVHotVideos:
    """çœŸå®çš„MissAVçƒ­æ¦œçˆ¬è™«"""
    
    def __init__(self):
        self.base_urls = [
            "https://missav.ws",
            "https://missav.com"
        ]
        
        # çƒ­æ¦œé¡µé¢URLæ˜ å°„
        self.hot_pages = {
            'en': '/dm22/en',                    # è‹±æ–‡é¡µé¢
            'chinese_subtitle': '/dm265/chinese-subtitle',  # ä¸­æ–‡å­—å¹•
            'new': '/dm514/new',                 # æœ€æ–°
            'release': '/dm588/release',         # å‘è¡Œ
            'uncensored_leak': '/dm621/uncensored-leak',    # æ— ç æµå‡º
            'today_hot': '/dm291/today-hot',     # ä»Šæ—¥çƒ­é—¨
            'weekly_hot': '/dm169/weekly-hot',   # æ¯å‘¨çƒ­é—¨
            'monthly_hot': '/dm257/monthly-hot', # æ¯æœˆçƒ­é—¨
            'siro': '/dm23/siro',               # SIROç³»åˆ—
            'luxu': '/dm20/luxu',               # LUXUç³»åˆ—
            'gana': '/dm17/gana'                # GANAç³»åˆ—
        }
        
        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }
        
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        
    def get_working_base_url(self) -> Optional[str]:
        """è·å–å¯ç”¨çš„åŸºç¡€URL"""
        for base_url in self.base_urls:
            try:
                debug_print(f"æµ‹è¯•è¿æ¥: {base_url}")
                response = self.session.get(base_url, timeout=10)
                if response.status_code == 200:
                    debug_print(f"âœ… è¿æ¥æˆåŠŸ: {base_url}")
                    return base_url
            except Exception as e:
                debug_print(f"âŒ è¿æ¥å¤±è´¥ {base_url}: {str(e)}")
                continue
        return None
    
    def crawl_hot_page(self, page_type: str = 'today_hot', page: int = 1) -> Dict:
        """
        çˆ¬å–æŒ‡å®šçƒ­æ¦œé¡µé¢
        
        Args:
            page_type: é¡µé¢ç±»å‹
            page: é¡µç 
            
        Returns:
            çˆ¬å–ç»“æœ
        """
        try:
            # è·å–å¯ç”¨çš„åŸºç¡€URL
            base_url = self.get_working_base_url()
            if not base_url:
                return {
                    "success": False,
                    "error": "æ— æ³•è¿æ¥åˆ°ä»»ä½•MissAVç½‘ç«™",
                    "results": []
                }
            
            # æ„å»ºç›®æ ‡URL
            if page_type not in self.hot_pages:
                page_type = 'today_hot'
            
            page_path = self.hot_pages[page_type]
            target_url = f"{base_url}{page_path}"
            
            # æ·»åŠ é¡µç å‚æ•°
            if page > 1:
                separator = '&' if '?' in target_url else '?'
                target_url += f"{separator}page={page}"
            
            debug_print(f"ğŸ” å¼€å§‹çˆ¬å–: {target_url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(target_url, timeout=15)
            response.raise_for_status()
            
            debug_print(f"âœ… é¡µé¢è·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
            debug_print(f"ğŸ“„ é¡µé¢å¤§å°: {len(response.content)} bytes")
            
            # è§£æé¡µé¢
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # æå–è§†é¢‘ä¿¡æ¯
            videos = self.extract_videos_from_soup(soup, base_url)
            
            if not videos:
                debug_print("âš ï¸ æœªæ‰¾åˆ°è§†é¢‘æ•°æ®ï¼Œå°è¯•è°ƒè¯•é¡µé¢ç»“æ„...")
                self.debug_page_structure(soup)
            
            return {
                "success": True,
                "page_type": page_type,
                "page": page,
                "target_url": target_url,
                "results": videos,
                "total_count": len(videos),
                "message": f"æˆåŠŸçˆ¬å–åˆ° {len(videos)} ä¸ªè§†é¢‘",
                "source": "real_crawl"
            }
            
        except requests.RequestException as e:
            return {
                "success": False,
                "error": f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                "results": []
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"çˆ¬å–å¤±è´¥: {str(e)}",
                "results": []
            }
    
    def extract_videos_from_soup(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """ä»BeautifulSoupå¯¹è±¡ä¸­æå–è§†é¢‘ä¿¡æ¯"""
        videos = []
        
        # å°è¯•å¤šç§å¯èƒ½çš„è§†é¢‘å®¹å™¨é€‰æ‹©å™¨
        selectors = [
            'div.thumbnail',
            'div.video-item',
            'div.item',
            'article',
            'div[class*="video"]',
            'div[class*="item"]',
            'a[href*="/"]'  # åŒ…å«é“¾æ¥çš„å…ƒç´ 
        ]
        
        for selector in selectors:
            video_elements = soup.select(selector)
            debug_print(f"ğŸ” å°è¯•é€‰æ‹©å™¨ '{selector}': æ‰¾åˆ° {len(video_elements)} ä¸ªå…ƒç´ ")
            
            if video_elements:
                for i, element in enumerate(video_elements[:50]):  # é™åˆ¶å¤„ç†æ•°é‡
                    video_info = self.extract_single_video_info(element, base_url)
                    if video_info:
                        videos.append(video_info)
                        if len(videos) >= 30:  # é™åˆ¶ç»“æœæ•°é‡
                            break
                
                if videos:
                    debug_print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æˆåŠŸæå–åˆ° {len(videos)} ä¸ªè§†é¢‘")
                    break
        
        return videos
    
    def extract_single_video_info(self, element, base_url: str) -> Optional[Dict]:
        """ä»å•ä¸ªå…ƒç´ ä¸­æå–è§†é¢‘ä¿¡æ¯"""
        try:
            video_info = {}
            
            # æŸ¥æ‰¾é“¾æ¥
            link_element = element.find('a') if element.name != 'a' else element
            if not link_element or not link_element.get('href'):
                return None
            
            href = link_element.get('href')
            if not href.startswith('http'):
                href = urljoin(base_url, href)
            
            video_info['url'] = href
            
            # æå–è§†é¢‘ä»£ç ï¼ˆä»URLä¸­ï¼‰
            video_code_match = re.search(r'/([a-zA-Z0-9]+-[0-9]+)', href, re.IGNORECASE)
            if video_code_match:
                video_info['video_code'] = video_code_match.group(1).upper()
            else:
                # å°è¯•ä»altå±æ€§æå–
                alt_text = link_element.get('alt', '')
                if alt_text:
                    code_match = re.search(r'([a-zA-Z0-9]+-[0-9]+)', alt_text, re.IGNORECASE)
                    if code_match:
                        video_info['video_code'] = code_match.group(1).upper()
                    else:
                        video_info['video_code'] = alt_text.upper()
                else:
                    video_info['video_code'] = href.split('/')[-1].upper() or 'UNKNOWN'
            
            # æå–æ ‡é¢˜ - ä¼˜å…ˆä»imgçš„altå±æ€§æˆ–é“¾æ¥æ–‡æœ¬ä¸­è·å–
            title = None
            
            # 1. å°è¯•ä»imgçš„altå±æ€§è·å–å®Œæ•´æ ‡é¢˜
            img_element = element.find('img')
            if img_element and img_element.get('alt'):
                alt_text = img_element.get('alt').strip()
                if len(alt_text) > 10:  # ç¡®ä¿æ˜¯å®Œæ•´æ ‡é¢˜è€Œä¸æ˜¯ä»£ç 
                    title = alt_text
            
            # 2. å°è¯•ä»é“¾æ¥æ–‡æœ¬è·å–
            if not title:
                link_text = link_element.get_text(strip=True)
                if link_text and len(link_text) > 10:
                    title = link_text
            
            # 3. å°è¯•ä»å…¶ä»–å…ƒç´ è·å–
            if not title:
                title_selectors = [
                    'div.my-2 a',  # åŸºäºè§‚å¯Ÿåˆ°çš„ç»“æ„
                    '.title', '.video-title', '.name',
                    'h5', 'h4', 'h3', 'h2', 'h1'
                ]
                
                for selector in title_selectors:
                    title_element = element.select_one(selector)
                    if title_element:
                        title_text = title_element.get_text(strip=True)
                        if title_text and len(title_text) > 5:
                            title = title_text
                            break
            
            # 4. å¤‡ç”¨æ–¹æ¡ˆ
            if not title:
                title = link_element.get('alt') or video_info['video_code']
            
            video_info['title'] = title.strip() if title else video_info['video_code']
            
            # æå–ç¼©ç•¥å›¾
            if img_element:
                # ä¼˜å…ˆä½¿ç”¨data-srcï¼Œç„¶åæ˜¯src
                img_src = img_element.get('data-src') or img_element.get('src')
                if img_src and not img_src.startswith('data:'):  # æ’é™¤base64å ä½ç¬¦
                    if not img_src.startswith('http'):
                        img_src = urljoin(base_url, img_src)
                    video_info['thumbnail'] = img_src
            
            # æå–æ—¶é•¿ - æŸ¥æ‰¾åŒ…å«æ—¶é—´æ ¼å¼çš„spanå…ƒç´ 
            duration_selectors = [
                'span.absolute.bottom-1.right-1',  # æ›´ç²¾ç¡®çš„é€‰æ‹©å™¨
                'span.absolute',  # åŸºäºè§‚å¯Ÿåˆ°çš„ç»“æ„
                '.duration', '.time', '.length',
                'span[class*="duration"]', 'div[class*="time"]'
            ]
            
            for selector in duration_selectors:
                duration_elements = element.select(selector)
                for duration_element in duration_elements:
                    duration_text = duration_element.get_text(strip=True)
                    # åŒ¹é…æ—¶é—´æ ¼å¼ï¼šHH:MM:SS æˆ– MM:SS
                    if re.match(r'\d{1,2}:\d{2}(:\d{2})?', duration_text):
                        video_info['duration'] = duration_text
                        break
                if video_info.get('duration'):
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¶é•¿ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰spanå…ƒç´ ä¸­çš„æ—¶é—´æ ¼å¼
            if not video_info.get('duration'):
                all_spans = element.find_all('span')
                for span in all_spans:
                    span_text = span.get_text(strip=True)
                    if re.match(r'\d{1,2}:\d{2}(:\d{2})?', span_text):
                        video_info['duration'] = span_text
                        break
            
            # æå–å…¶ä»–ä¿¡æ¯
            video_info['publish_date'] = datetime.now().strftime('%Y-%m-%d')
            video_info['source'] = 'real_crawl'
            
            # éªŒè¯å¿…è¦å­—æ®µ
            if video_info.get('video_code') and video_info.get('title'):
                return video_info
            
        except Exception as e:
            debug_print(f"âš ï¸ æå–è§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return None
    
    def debug_page_structure(self, soup: BeautifulSoup):
        """è°ƒè¯•é¡µé¢ç»“æ„"""
        debug_print("\nğŸ” è°ƒè¯•é¡µé¢ç»“æ„:")
        
        # æŸ¥æ‰¾å¯èƒ½åŒ…å«è§†é¢‘çš„å…ƒç´ 
        potential_containers = [
            'div[class*="video"]',
            'div[class*="item"]', 
            'div[class*="card"]',
            'div[class*="thumb"]',
            'article',
            'section'
        ]
        
        for selector in potential_containers:
            elements = soup.select(selector)
            if elements:
                debug_print(f"  - {selector}: {len(elements)} ä¸ªå…ƒç´ ")
                if len(elements) > 0:
                    first_element = elements[0]
                    debug_print(f"    ç¬¬ä¸€ä¸ªå…ƒç´ çš„ç±»: {first_element.get('class')}")
                    debug_print(f"    ç¬¬ä¸€ä¸ªå…ƒç´ çš„æ–‡æœ¬ç‰‡æ®µ: {first_element.get_text()[:100]}...")
        
        # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        video_links = [link for link in links if re.search(r'/[A-Z0-9]+-[0-9]+', link.get('href', ''))]
        debug_print(f"  - æ‰¾åˆ° {len(video_links)} ä¸ªå¯èƒ½çš„è§†é¢‘é“¾æ¥")
        
        if video_links:
            debug_print(f"    ç¤ºä¾‹é“¾æ¥: {video_links[0].get('href')}")


class EnhancedHotVideos:
    """å¢å¼ºçš„çƒ­æ¦œæ¨¡å—"""
    
    def __init__(self):
        self.real_crawler = RealMissAVHotVideos()
        self.sort_filter = SortFilterModule()
        # ç§»é™¤è™šæ„æ•°æ®å¤‡ç”¨æºï¼Œåªä½¿ç”¨çœŸå®æ•°æ®
        
        # åˆ†ç±»æ˜ å°„åˆ°çˆ¬è™«é¡µé¢ç±»å‹
        self.category_mapping = {
            'daily': 'today_hot',
            'weekly': 'weekly_hot', 
            'monthly': 'monthly_hot',
            'new': 'new',
            'chinese_subtitle': 'chinese_subtitle',
            'uncensored_leak': 'uncensored_leak',
            'siro': 'siro',
            'luxu': 'luxu',
            'gana': 'gana'
        }
    
    def get_hot_videos_with_filters(self, category: str = "daily", page: int = 1,
                                  sort: Optional[str] = None, 
                                  filter_type: Optional[str] = None) -> Dict:
        """
        è·å–å¸¦æ’åºå’Œè¿‡æ»¤å™¨çš„çƒ­æ¦œè§†é¢‘
        
        Args:
            category: çƒ­æ¦œåˆ†ç±»
            page: é¡µç 
            sort: æ’åºæ–¹å¼
            filter_type: è¿‡æ»¤å™¨ç±»å‹
            
        Returns:
            çƒ­æ¦œè§†é¢‘ç»“æœ
        """
        try:
            debug_print(f"ğŸ”¥ å¼€å§‹è·å–çƒ­æ¦œ: category={category}, page={page}, sort={sort}, filter={filter_type}")
            
            # æ˜ å°„åˆ†ç±»åˆ°çˆ¬è™«é¡µé¢ç±»å‹
            page_type = self.category_mapping.get(category, 'today_hot')
            
            # å¦‚æœæœ‰è¿‡æ»¤å™¨ï¼Œä¼˜å…ˆä½¿ç”¨è¿‡æ»¤å™¨å¯¹åº”çš„é¡µé¢
            if filter_type and filter_type in self.category_mapping:
                page_type = filter_type
            
            # å°è¯•çœŸå®çˆ¬è™«
            result = self.real_crawler.crawl_hot_page(page_type, page)
            
            if result.get("success") and result.get("results"):
                videos = result.get("results", [])
                
                # åº”ç”¨å®¢æˆ·ç«¯æ’åºå’Œè¿‡æ»¤
                if filter_type and filter_type not in self.category_mapping:
                    videos = self.sort_filter.apply_client_side_filtering(videos, filter_type)
                
                if sort:
                    videos = self.sort_filter.apply_client_side_sorting(videos, sort)
                
                result["results"] = videos
                result["total_count"] = len(videos)
                result["applied_sort"] = sort
                result["applied_filter"] = filter_type
                result["category"] = category
                
                return result
            
            # ä¸å†ä½¿ç”¨è™šæ„æ•°æ®å¤‡ç”¨æºï¼Œç›´æ¥è¿”å›å¤±è´¥ç»“æœ
            debug_print("âŒ çœŸå®çˆ¬è™«å¤±è´¥ï¼Œä¸ä½¿ç”¨è™šæ„æ•°æ®")
            return {
                "success": False,
                "category": category,
                "page": page,
                "error": "æ— æ³•ä»çœŸå®æ•°æ®æºè·å–çƒ­æ¦œæ•°æ®",
                "results": [],
                "total_count": 0
            }
            
        except Exception as e:
            return {
                "success": False,
                "category": category,
                "page": page,
                "error": f"è·å–çƒ­æ¦œå¤±è´¥: {str(e)}",
                "results": []
            }
    
    def test_all_hot_pages(self) -> Dict:
        """æµ‹è¯•æ‰€æœ‰çƒ­æ¦œé¡µé¢"""
        results = {}
        
        print("ğŸ§ª å¼€å§‹æµ‹è¯•æ‰€æœ‰çƒ­æ¦œé¡µé¢...")
        
        for page_name, page_path in self.real_crawler.hot_pages.items():
            print(f"\n--- æµ‹è¯• {page_name} ({page_path}) ---")
            
            result = self.real_crawler.crawl_hot_page(page_name, 1)
            
            results[page_name] = {
                "success": result.get("success", False),
                "count": len(result.get("results", [])),
                "error": result.get("error") if not result.get("success") else None,
                "url": result.get("target_url")
            }
            
            if result.get("success"):
                videos = result.get("results", [])
                print(f"âœ… æˆåŠŸ: {len(videos)} ä¸ªè§†é¢‘")
                if videos:
                    print(f"   ç¤ºä¾‹: {videos[0].get('video_code')} - {videos[0].get('title', '')[:50]}...")
            else:
                print(f"âŒ å¤±è´¥: {result.get('error')}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(random.uniform(1, 3))
        
        return results
    
    def format_response(self, result: Dict) -> str:
        """æ ¼å¼åŒ–å“åº”ä¸ºå¯è¯»æ–‡æœ¬"""
        if not result.get("success"):
            error_msg = f"âŒ è·å–çƒ­æ¦œå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            if result.get("real_crawl_error"):
                error_msg += f"\n   çœŸå®çˆ¬è™«é”™è¯¯: {result['real_crawl_error']}"
            if result.get("fallback_error"):
                error_msg += f"\n   å¤‡ç”¨æ•°æ®æºé”™è¯¯: {result['fallback_error']}"
            return error_msg
        
        category = result.get("category", "daily")
        videos = result.get("results", [])
        source = result.get("source", "unknown")
        
        response = f"ğŸ”¥ MissAV {self._get_category_name(category)} çƒ­æ¦œ\n"
        response += f"æ•°æ®æº: {source}\n"
        
        if result.get("target_url"):
            response += f"çˆ¬å–URL: {result['target_url']}\n"
        
        if result.get("applied_sort"):
            response += f"æ’åº: {self.sort_filter.get_sort_name(result['applied_sort'])}\n"
        
        if result.get("applied_filter"):
            response += f"è¿‡æ»¤å™¨: {self.sort_filter.get_filter_name(result['applied_filter'])}\n"
        
        response += f"è§†é¢‘æ•°é‡: {len(videos)}\n\n"
        
        for i, video in enumerate(videos[:10], 1):  # æ˜¾ç¤ºå‰10ä¸ª
            response += f"{i}. {video.get('video_code', 'N/A')} - {video.get('title', 'N/A')[:50]}...\n"
            if video.get('duration'):
                response += f"   æ—¶é•¿: {video['duration']}\n"
            response += f"   é“¾æ¥: {video.get('url', 'N/A')}\n"
            if video.get('thumbnail'):
                response += f"   ç¼©ç•¥å›¾: {video['thumbnail']}\n"
            response += "\n"
        
        if len(videos) > 10:
            response += f"... è¿˜æœ‰ {len(videos) - 10} ä¸ªè§†é¢‘\n"
        
        return response
    
    def _get_category_name(self, category: str) -> str:
        """è·å–åˆ†ç±»ä¸­æ–‡åç§°"""
        names = {
            "daily": "æ¯æ—¥çƒ­é—¨",
            "weekly": "æ¯å‘¨çƒ­é—¨",
            "monthly": "æ¯æœˆçƒ­é—¨", 
            "new": "æœ€æ–°",
            "chinese_subtitle": "ä¸­æ–‡å­—å¹•",
            "uncensored_leak": "æ— ç æµå‡º",
            "siro": "SIROç³»åˆ—",
            "luxu": "LUXUç³»åˆ—",
            "gana": "GANAç³»åˆ—"
        }
        return names.get(category, category)


def test_enhanced_hot_videos():
    """æµ‹è¯•å¢å¼ºçƒ­æ¦œåŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•å¢å¼ºçƒ­æ¦œåŠŸèƒ½")
    print("=" * 60)
    
    enhanced = EnhancedHotVideos()
    
    # æµ‹è¯•å•ä¸ªé¡µé¢
    print("\n--- æµ‹è¯•å•ä¸ªçƒ­æ¦œé¡µé¢ ---")
    result = enhanced.get_hot_videos_with_filters("daily", 1)
    print(f"æ¯æ—¥çƒ­æ¦œ: æˆåŠŸ={result.get('success')}, æ•°é‡={len(result.get('results', []))}")
    if result.get('success') and result.get('results'):
        video = result['results'][0]
        print(f"ç¤ºä¾‹è§†é¢‘: {video.get('video_code')} - {video.get('title', '')[:50]}...")
    
    # æµ‹è¯•æ‰€æœ‰é¡µé¢
    print("\n--- æµ‹è¯•æ‰€æœ‰çƒ­æ¦œé¡µé¢ ---")
    all_results = enhanced.test_all_hot_pages()
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    success_count = sum(1 for r in all_results.values() if r['success'])
    total_count = len(all_results)
    
    print(f"æˆåŠŸ: {success_count}/{total_count} ä¸ªé¡µé¢")
    
    for page_name, info in all_results.items():
        status = "âœ…" if info["success"] else "âŒ"
        print(f"{status} {page_name}: {info['count']} ä¸ªè§†é¢‘")
        if info.get("error"):
            print(f"    é”™è¯¯: {info['error']}")


if __name__ == "__main__":
    test_enhanced_hot_videos()