#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MissAV ç»Ÿä¸€æœç´¢æ¨¡å—
ç‹¬ç«‹å°è£…çš„æœç´¢åŠŸèƒ½ï¼Œä¾›æœç´¢ã€çƒ­æ¦œç­‰åŠŸèƒ½ä½¿ç”¨
æ”¯æŒæ’åºå‚æ•°ä¸è¿‡æ»¤å™¨å‚æ•°çš„ç»„åˆä½¿ç”¨
"""

import sys
import re
import requests
import time
from pathlib import Path
from urllib.parse import quote, urljoin, urlparse, parse_qs
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup

from .debug_utils import debug_print

# æ·»åŠ å½“å‰ç›®å½•åˆ° Python è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from consts import HEADERS
from sort_filter_module import SortFilterModule


class UnifiedSearchModule:
    """ç»Ÿä¸€æœç´¢æ¨¡å—"""
    
    def __init__(self):
        self.base_url = "https://missav.ws"
        self.headers = HEADERS.copy()
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.sort_filter = SortFilterModule()
        
        # æ·»åŠ æ›´å¤šçš„User-Agentè½®æ¢å’Œåçˆ¬è™«æªæ–½
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        self.current_ua_index = 0
        
        # æ·»åŠ æ›´å¤šåçˆ¬è™«å¤´éƒ¨
        self.session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        })
    
    def _rotate_user_agent(self):
        """è½®æ¢User-Agent"""
        self.current_ua_index = (self.current_ua_index + 1) % len(self.user_agents)
        self.session.headers.update({
            'User-Agent': self.user_agents[self.current_ua_index]
        })
    
    def _build_search_url_candidates(self, keyword: str, page: int = 1, 
                                   sort: Optional[str] = None, filter_type: Optional[str] = None) -> List[str]:
        """æ„å»ºå¤šä¸ªå€™é€‰æœç´¢URL"""
        candidates = []
        
        # å¤„ç†å…³é”®è¯ç¼–ç  - æ”¯æŒå¤šå…³é”®è¯æœç´¢
        from urllib.parse import quote
        
        # æ£€æµ‹æ˜¯å¦æ˜¯å¤šå…³é”®è¯æœç´¢
        keywords = keyword.strip().split()
        if len(keywords) > 1:
            # å¤šå…³é”®è¯æœç´¢ï¼šä¼˜å…ˆä½¿ç”¨æœ‰æ•ˆçš„ç»„åˆæ–¹å¼
            keyword_variants = []
            
            # 1. å¦‚æœå…³é”®è¯æ•°é‡<=3ï¼Œå°è¯•ç”¨+è¿æ¥ï¼ˆæœ€æœ‰æ•ˆï¼‰
            if len(keywords) <= 3:
                keyword_variants.append('+'.join(keywords))
            
            # 2. å°è¯•å‰ä¸¤ä¸ªå…³é”®è¯ç”¨+è¿æ¥
            if len(keywords) >= 2:
                keyword_variants.append('+'.join(keywords[:2]))
            
            # 3. åªä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯ä½œä¸ºå›é€€
            keyword_variants.append(keywords[0])
            
            # 4. åŸå§‹å…³é”®è¯ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰ä½œä¸ºæœ€åå°è¯•
            keyword_variants.append(keyword.strip())
        else:
            # å•å…³é”®è¯æœç´¢
            keyword_variants = [keyword.strip()]
        
        # æ„å»ºæŸ¥è¯¢å‚æ•°
        params = []
        if page > 1:
            params.append(f"page={page}")
        if sort and sort in self.sort_filter.sort_params:
            sort_value = self.sort_filter.sort_params[sort]
            if sort_value:
                params.append(f"sort={sort_value}")
        if filter_type and filter_type in self.sort_filter.filter_params:
            filter_value = self.sort_filter.filter_params[filter_type]
            if filter_value:
                params.append(f"filters={filter_value}")
        
        param_string = '?' + '&'.join(params) if params else ''
        
        # ä¸ºæ¯ä¸ªå…³é”®è¯å˜ä½“æ„å»ºURL
        for variant in keyword_variants:
            encoded_keyword = quote(variant)
            
            # å€™é€‰URLæ ¼å¼1: ä¼ ç»Ÿæœç´¢æ ¼å¼
            traditional_url = f"{self.base_url}/search/{encoded_keyword}{param_string}"
            candidates.append(traditional_url)
            
            # å€™é€‰URLæ ¼å¼2: ç®€åŒ–æœç´¢æ ¼å¼
            simple_search_url = f"{self.base_url}/{encoded_keyword}{param_string}"
            candidates.append(simple_search_url)
            
            # å€™é€‰URLæ ¼å¼3: genresæ ¼å¼ï¼ˆå¦‚æœæœ‰è¿‡æ»¤å™¨ï¼‰
            if filter_type and filter_type != 'all':
                genres_url = f"{self.base_url}/genres/{encoded_keyword}{param_string}"
                candidates.append(genres_url)
        
        # å€™é€‰URLæ ¼å¼4: å¸¦dmä»£ç çš„genresæ ¼å¼ï¼ˆåªä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯å˜ä½“ï¼‰
        first_variant = keyword_variants[0]
        encoded_first = quote(first_variant)
        dm_codes = ["dm4416", "dm54", "dm22"]
        for dm_code in dm_codes:
            for lang in ["zh", "en"]:
                dm_url = f"{self.base_url}/{dm_code}/{lang}/genres/{encoded_first}{param_string}"
                candidates.append(dm_url)
        
        return candidates
    
    def _is_multi_keyword_search(self, keyword: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯å¤šå…³é”®è¯æœç´¢"""
        keyword = keyword.strip()
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç©ºæ ¼ã€+å·æˆ–é€—å·åˆ†éš”çš„å¤šä¸ªå…³é”®è¯
        return (' ' in keyword or '+' in keyword or ',' in keyword) and len(keyword) > 1
    
    def _extract_first_keyword(self, keyword: str) -> str:
        """ä»å¤šå…³é”®è¯ä¸­æå–ç¬¬ä¸€ä¸ªå…³é”®è¯"""
        keyword = keyword.strip()
        # æŒ‰ä¸åŒåˆ†éš”ç¬¦åˆ†å‰²ï¼Œå–ç¬¬ä¸€ä¸ª
        if ' ' in keyword:
            return keyword.split()[0]
        elif '+' in keyword:
            return keyword.split('+')[0]
        elif ',' in keyword:
            return keyword.split(',')[0]
        else:
            return keyword
    
    def search_with_filters(self, keyword: str, page: int = 1, 
                           sort: Optional[str] = None, filter_type: Optional[str] = None,
                           max_results: int = 20, max_pages: int = 1, 
                           enhanced_info: bool = False) -> Dict:
        """
        ä½¿ç”¨æ’åºå’Œè¿‡æ»¤å™¨è¿›è¡Œæœç´¢
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            page: èµ·å§‹é¡µç 
            sort: æ’åºç±»å‹
            filter_type: è¿‡æ»¤å™¨ç±»å‹
            max_results: æ¯é¡µæœ€å¤§ç»“æœæ•°
            max_pages: æœ€å¤§æœç´¢é¡µæ•°
            enhanced_info: æ˜¯å¦æå–å¢å¼ºä¿¡æ¯ï¼ˆæ¼”å‘˜ã€æ ‡ç­¾ã€ç³»åˆ—ç­‰ï¼‰
            
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        all_results = []
        actual_pages = 0
        
        try:
            for current_page in range(page, page + max_pages):
                # æ„å»ºå¤šä¸ªå€™é€‰æœç´¢URL
                url_candidates = self._build_search_url_candidates(keyword, current_page, sort, filter_type)
                
                response = None
                successful_url = None
                
                # å°è¯•æ¯ä¸ªå€™é€‰URL
                for i, search_url in enumerate(url_candidates):
                    debug_print(f"ğŸ” å°è¯•æœç´¢URL {i+1}/{len(url_candidates)}: {search_url}")
                    
                    # è½®æ¢User-Agent
                    self._rotate_user_agent()
                    
                    # å‘é€è¯·æ±‚ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
                    max_retries = 2  # å‡å°‘é‡è¯•æ¬¡æ•°ï¼Œå› ä¸ºæœ‰å¤šä¸ªURLå¯ä»¥å°è¯•
                    success = False
                    
                    for retry in range(max_retries):
                        try:
                            response = self.session.get(search_url, timeout=30)
                            response.raise_for_status()
                            successful_url = search_url
                            success = True
                            debug_print(f"âœ… URL {i+1} è¯·æ±‚æˆåŠŸ")
                            break
                        except requests.exceptions.HTTPError as e:
                            if e.response.status_code in [403, 404] and retry < max_retries - 1:
                                debug_print(f"âš ï¸ é‡åˆ°{e.response.status_code}é”™è¯¯ï¼Œç­‰å¾…{1 + retry}ç§’åé‡è¯•...")
                                time.sleep(1 + retry)
                                self._rotate_user_agent()
                                continue
                            else:
                                debug_print(f"âŒ URL {i+1} å¤±è´¥: {e.response.status_code}")
                                break
                        except Exception as e:
                            debug_print(f"âŒ URL {i+1} å¼‚å¸¸: {str(e)}")
                            break
                    
                    if success:
                        break
                
                # å¦‚æœæ‰€æœ‰URLéƒ½å¤±è´¥äº†
                if not response or not successful_url:
                    debug_print(f"âŒ æ‰€æœ‰æœç´¢URLéƒ½å¤±è´¥äº†")
                    break
                
                # è§£æç»“æœ
                page_results = self._parse_search_page(response.text, keyword, enhanced_info, max_results)
                
                if page_results:
                    all_results.extend(page_results)
                    actual_pages += 1
                    debug_print(f"âœ… ç¬¬{current_page}é¡µæ‰¾åˆ° {len(page_results)} ä¸ªç»“æœ")
                else:
                    debug_print(f"âš ï¸ ç¬¬{current_page}é¡µæ²¡æœ‰æ‰¾åˆ°ç»“æœ")
                    
                    # å¦‚æœæ˜¯å¤šå…³é”®è¯æœç´¢ä¸”ç¬¬ä¸€é¡µå°±æ²¡æœ‰ç»“æœï¼Œå°è¯•å›é€€åˆ°å•å…³é”®è¯æœç´¢
                    if current_page == page and self._is_multi_keyword_search(keyword):
                        debug_print("ğŸ”„ å¤šå…³é”®è¯æœç´¢æ— ç»“æœï¼Œå°è¯•å›é€€åˆ°å•å…³é”®è¯æœç´¢")
                        first_keyword = self._extract_first_keyword(keyword)
                        debug_print(f"ğŸ” ä½¿ç”¨ç¬¬ä¸€ä¸ªå…³é”®è¯è¿›è¡Œæœç´¢: '{first_keyword}'")
                        
                        # æ„å»ºå•å…³é”®è¯æœç´¢URL
                        fallback_candidates = self._build_search_url_candidates(first_keyword, current_page, sort, filter_type)
                        
                        # å°è¯•å•å…³é”®è¯æœç´¢
                        for fallback_url in fallback_candidates[:3]:  # åªå°è¯•å‰3ä¸ªURL
                            debug_print(f"ğŸ” å°è¯•å›é€€æœç´¢URL: {fallback_url}")
                            try:
                                fallback_response = self.session.get(fallback_url, timeout=30)
                                fallback_response.raise_for_status()
                                
                                fallback_results = self._parse_search_page(fallback_response.text, first_keyword, enhanced_info, max_results)
                                if fallback_results:
                                    debug_print(f"âœ… å›é€€æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(fallback_results)} ä¸ªç»“æœ")
                                    all_results.extend(fallback_results)
                                    actual_pages += 1
                                    break
                            except Exception as e:
                                debug_print(f"âš ï¸ å›é€€æœç´¢å¤±è´¥: {str(e)}")
                                continue
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œåœæ­¢æœç´¢
                    if not all_results:
                        debug_print("âŒ æ‰€æœ‰æœç´¢ç­–ç•¥éƒ½å¤±è´¥ï¼Œåœæ­¢æœç´¢")
                        break
                
                # é™åˆ¶æ€»ç»“æœæ•°
                if len(all_results) >= max_results:
                    all_results = all_results[:max_results]
                    break
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                if current_page < page + max_pages - 1:
                    time.sleep(1)
            
            # ä¿®å¤æ‰€æœ‰ç»“æœçš„å°é¢å›¾ç‰‡URL
            for video in all_results:
                video_code = video.get("video_code")
                if video_code and (not video.get("thumbnail") or video.get("thumbnail").startswith('data:')):
                    video["thumbnail"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
            
            # å¦‚æœéœ€è¦å¢å¼ºä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªç»“æœè·å–è¯¦ç»†ä¿¡æ¯
            debug_print(f"ğŸ” å¢å¼ºä¿¡æ¯å‚æ•°: enhanced_info={enhanced_info}, ç»“æœæ•°é‡: {len(all_results)}")
            if enhanced_info and all_results:
                debug_print(f"ğŸ” å¼€å§‹è·å– {len(all_results)} ä¸ªè§†é¢‘çš„å¢å¼ºä¿¡æ¯...")
                all_results = self._enrich_video_results(all_results)
            elif enhanced_info and not all_results:
                debug_print("âš ï¸ å¢å¼ºä¿¡æ¯å·²å¯ç”¨ï¼Œä½†æ²¡æœ‰æœç´¢ç»“æœ")
            elif not enhanced_info:
                debug_print("â„¹ï¸ å¢å¼ºä¿¡æ¯æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯")
            
            return {
                "success": True,
                "keyword": keyword,
                "page": page,
                "sort": sort,
                "filter_type": filter_type,
                "sort_name": self.sort_filter.get_sort_name(sort) if sort else None,
                "filter_name": self.sort_filter.get_filter_name(filter_type) if filter_type else None,
                "results": all_results,
                "total_count": len(all_results),
                "actual_pages": actual_pages,
                "max_pages": max_pages,
                "enhanced_info": enhanced_info,
                "message": f"æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(all_results)} ä¸ªç»“æœ" + (" (åŒ…å«å¢å¼ºä¿¡æ¯)" if enhanced_info else "")
            }
            
        except Exception as e:
            debug_print(f"âŒ æœç´¢å‡ºé”™: {str(e)}")
            return {
                "success": False,
                "keyword": keyword,
                "page": page,
                "sort": sort,
                "filter_type": filter_type,
                "error": f"æœç´¢å¤±è´¥: {str(e)}",
                "results": [],
                "total_count": 0
            }
    
    def get_hot_videos_with_filters(self, category: str = "daily", page: int = 1,
                                   sort: Optional[str] = None, filter_type: Optional[str] = None,
                                   include_cover: bool = True, include_title: bool = True,
                                   max_results: int = 20, max_pages: int = 1,
                                   enhanced_info: bool = False) -> Dict:
        """
        è·å–å¸¦è¿‡æ»¤å™¨çš„çƒ­æ¦œè§†é¢‘ - ä¸SearchWithFiltersçœ‹é½
        
        Args:
            category: çƒ­æ¦œç±»å‹
            page: é¡µç 
            sort: æ’åºç±»å‹
            filter_type: è¿‡æ»¤å™¨ç±»å‹
            include_cover: æ˜¯å¦è¿”å›è§†é¢‘å°é¢å›¾ç‰‡URL
            include_title: æ˜¯å¦è¿”å›è§†é¢‘å®Œæ•´æ ‡é¢˜
            max_results: æ¯é¡µæœ€å¤§ç»“æœæ•°é‡
            max_pages: æœ€å¤§æœç´¢é¡µæ•°
            enhanced_info: æ˜¯å¦æå–å¢å¼ºä¿¡æ¯ï¼ˆæ¼”å‘˜ã€æ ‡ç­¾ã€ç³»åˆ—ç­‰ï¼‰
            
        Returns:
            çƒ­æ¦œç»“æœå­—å…¸
        """
        all_results = []
        actual_pages = 0
        
        try:
            for current_page in range(page, page + max_pages):
                # æ„å»ºçƒ­æ¦œURL
                hot_url = self.sort_filter.build_hot_videos_url(
                    self.base_url, category, current_page, sort, filter_type
                )
                
                debug_print(f"ğŸ”¥ çƒ­æ¦œURL: {hot_url}")
                
                # è½®æ¢User-Agent
                self._rotate_user_agent()
                
                # å‘é€è¯·æ±‚ï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
                max_retries = 3
                for retry in range(max_retries):
                    try:
                        response = self.session.get(hot_url, timeout=30)
                        response.raise_for_status()
                        break
                    except requests.exceptions.HTTPError as e:
                        if e.response.status_code == 403 and retry < max_retries - 1:
                            debug_print(f"âš ï¸ é‡åˆ°403é”™è¯¯ï¼Œç­‰å¾…{2 ** retry}ç§’åé‡è¯•...")
                            time.sleep(2 ** retry)
                            self._rotate_user_agent()
                            continue
                        else:
                            raise
                
                # è§£æç»“æœ
                page_results = self._parse_hot_videos_page(response.text, category, enhanced_info, max_results)
                
                if page_results:
                    all_results.extend(page_results)
                    actual_pages += 1
                    debug_print(f"âœ… ç¬¬{current_page}é¡µæ‰¾åˆ° {len(page_results)} ä¸ªç»“æœ")
                else:
                    debug_print(f"âš ï¸ ç¬¬{current_page}é¡µæ²¡æœ‰æ‰¾åˆ°ç»“æœï¼Œåœæ­¢æœç´¢")
                    break
                
                # é™åˆ¶æ€»ç»“æœæ•°
                if len(all_results) >= max_results:
                    all_results = all_results[:max_results]
                    break
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«å°
                if current_page < page + max_pages - 1:
                    time.sleep(1)
            
            # ä¿®å¤æ‰€æœ‰ç»“æœçš„å°é¢å›¾ç‰‡URL
            for video in all_results:
                video_code = video.get("video_code")
                if video_code and (not video.get("thumbnail") or video.get("thumbnail").startswith('data:')):
                    video["thumbnail"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
            
            # å¦‚æœéœ€è¦å¢å¼ºä¿¡æ¯ï¼Œä¸ºæ¯ä¸ªç»“æœè·å–è¯¦ç»†ä¿¡æ¯
            debug_print(f"ğŸ” å¢å¼ºä¿¡æ¯å‚æ•°: enhanced_info={enhanced_info}, ç»“æœæ•°é‡: {len(all_results)}")
            if enhanced_info and all_results:
                debug_print(f"ğŸ” å¼€å§‹è·å– {len(all_results)} ä¸ªè§†é¢‘çš„å¢å¼ºä¿¡æ¯...")
                all_results = self._enrich_video_results(all_results)
            elif enhanced_info and not all_results:
                debug_print("âš ï¸ å¢å¼ºä¿¡æ¯å·²å¯ç”¨ï¼Œä½†æ²¡æœ‰çƒ­æ¦œç»“æœ")
            elif not enhanced_info:
                debug_print("â„¹ï¸ å¢å¼ºä¿¡æ¯æœªå¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ä¿¡æ¯")
            
            return {
                "success": True,
                "category": category,
                "page": page,
                "sort": sort,
                "filter_type": filter_type,
                "sort_name": self.sort_filter.get_sort_name(sort) if sort else None,
                "filter_name": self.sort_filter.get_filter_name(filter_type) if filter_type else None,
                "results": all_results,
                "total_count": len(all_results),
                "actual_pages": actual_pages,
                "max_pages": max_pages,
                "enhanced_info": enhanced_info,
                "source": "real_crawl",
                "message": f"è·å–çƒ­æ¦œæˆåŠŸï¼Œæ‰¾åˆ° {len(all_results)} ä¸ªè§†é¢‘" + (" (åŒ…å«å¢å¼ºä¿¡æ¯)" if enhanced_info else "")
            }
            
        except Exception as e:
            debug_print(f"âŒ è·å–çƒ­æ¦œå‡ºé”™: {str(e)}")
            return {
                "success": False,
                "category": category,
                "page": page,
                "sort": sort,
                "filter_type": filter_type,
                "error": f"è·å–çƒ­æ¦œå¤±è´¥: {str(e)}",
                "results": [],
                "total_count": 0
            }
    
    def _parse_search_page(self, html_content: str, keyword: str, enhanced_info: bool = False, max_results: int = 100) -> List[Dict]:
        """è§£ææœç´¢ç»“æœé¡µé¢ - å¢å¼ºç‰ˆ"""
        results = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            debug_print(f"ğŸ” å¼€å§‹è§£ææœç´¢é¡µé¢ï¼Œé¡µé¢é•¿åº¦: {len(html_content)}")
            
            # æ–¹æ³•1: å¯»æ‰¾MissAVç‰¹å®šçš„è§†é¢‘å®¹å™¨
            # MissAVé€šå¸¸ä½¿ç”¨ç‰¹å®šçš„classåç§°
            video_containers = soup.find_all(['div'], class_=re.compile(r'.*thumbnail.*|.*video.*|.*item.*|.*card.*|.*grid.*', re.I))
            debug_print(f"ğŸ” æ‰¾åˆ° {len(video_containers)} ä¸ªå¯èƒ½çš„è§†é¢‘å®¹å™¨")
            
            seen_codes = set()  # ç”¨äºå»é‡
            for container in video_containers:
                video_info = self._extract_video_info_from_container(container, enhanced_info)
                if video_info and self._is_valid_video_info(video_info):
                    # å»é‡ï¼šåŸºäºè§†é¢‘ä»£ç 
                    video_code = video_info.get('video_code', '')
                    if video_code and video_code not in seen_codes:
                        results.append(video_info)
                        seen_codes.add(video_code)
                        debug_print(f"âœ… æå–åˆ°å”¯ä¸€è§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_code})")
                    elif video_code in seen_codes:
                        debug_print(f"âš ï¸ è·³è¿‡é‡å¤è§†é¢‘: {video_code}")
            
            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1æ²¡æœ‰ç»“æœï¼Œå°è¯•å¯»æ‰¾æ‰€æœ‰è§†é¢‘é“¾æ¥
            if not results:
                debug_print("ğŸ” æ–¹æ³•1æ— ç»“æœï¼Œå°è¯•æ–¹æ³•2ï¼šå¯»æ‰¾è§†é¢‘é“¾æ¥")
                all_links = soup.find_all('a', href=True)
                debug_print(f"ğŸ” æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
                
                seen_codes = set()  # ç”¨äºå»é‡
                for link in all_links:
                    href = link.get('href', '')
                    if self._is_missav_video_link(href):
                        video_info = self._extract_video_info_from_link_enhanced(link)
                        if video_info and self._is_valid_video_info(video_info):
                            # å»é‡ï¼šåŸºäºè§†é¢‘ä»£ç 
                            video_code = video_info.get('video_code', '')
                            if video_code and video_code not in seen_codes:
                                results.append(video_info)
                                seen_codes.add(video_code)
                                debug_print(f"âœ… ä»é“¾æ¥æå–åˆ°å”¯ä¸€è§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_code})")
                            elif video_code in seen_codes:
                                debug_print(f"âš ï¸ è·³è¿‡é‡å¤è§†é¢‘: {video_code}")
            
            # æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–
            if not results:
                debug_print("ğŸ” æ–¹æ³•2æ— ç»“æœï¼Œå°è¯•æ–¹æ³•3ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–")
                raw_results = self._extract_videos_with_regex(html_content)
                # éªŒè¯æå–çš„ç»“æœ
                for video_info in raw_results:
                    if self._is_valid_video_info(video_info):
                        results.append(video_info)
                        debug_print(f"âœ… æ­£åˆ™æå–åˆ°æœ‰æ•ˆè§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_info.get('video_code', 'æ— ä»£ç ')})")
                    else:
                        debug_print(f"âš ï¸ æ­£åˆ™æå–åˆ°æ— æ•ˆè§†é¢‘ï¼Œå·²è·³è¿‡: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_info.get('video_code', 'æ— ä»£ç ')})")
            
            # æ–¹æ³•4: æŸ¥æ‰¾JavaScriptä¸­çš„æ•°æ®
            if not results:
                debug_print("ğŸ” æ–¹æ³•3æ— ç»“æœï¼Œå°è¯•æ–¹æ³•4ï¼šJavaScriptæ•°æ®æå–")
                raw_results = self._extract_videos_from_javascript(html_content)
                # éªŒè¯æå–çš„ç»“æœ
                for video_info in raw_results:
                    if self._is_valid_video_info(video_info):
                        results.append(video_info)
                        debug_print(f"âœ… JSæå–åˆ°æœ‰æ•ˆè§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_info.get('video_code', 'æ— ä»£ç ')})")
                    else:
                        debug_print(f"âš ï¸ JSæå–åˆ°æ— æ•ˆè§†é¢‘ï¼Œå·²è·³è¿‡: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_info.get('video_code', 'æ— ä»£ç ')})")
            
            debug_print(f"ğŸ¯ æœ€ç»ˆæå–åˆ° {len(results)} ä¸ªè§†é¢‘ç»“æœ")
            
        except Exception as e:
            debug_print(f"âŒ è§£ææœç´¢é¡µé¢å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return results[:max_results]  # æ ¹æ®å‚æ•°é™åˆ¶ç»“æœæ•°é‡
    
    def _parse_hot_videos_page(self, html_content: str, category: str, enhanced_info: bool = False, max_results: int = 100) -> List[Dict]:
        """è§£æçƒ­æ¦œé¡µé¢ - æ”¯æŒå¢å¼ºä¿¡æ¯"""
        results = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            debug_print(f"ğŸ” å¼€å§‹è§£æçƒ­æ¦œé¡µé¢ï¼Œé¡µé¢é•¿åº¦: {len(html_content)}")
            
            # çƒ­æ¦œé¡µé¢é€šå¸¸æœ‰ç‰¹å®šçš„ç»“æ„
            # å°è¯•å¤šç§è§£ææ–¹æ³•
            
            # æ–¹æ³•1: å¯»æ‰¾çƒ­æ¦œç‰¹å®šçš„å®¹å™¨
            hot_containers = soup.find_all(['div', 'section'], class_=re.compile(r'.*hot.*|.*trend.*|.*popular.*', re.I))
            
            for container in hot_containers:
                video_cards = container.find_all(['div', 'article'], class_=re.compile(r'.*video.*|.*item.*', re.I))
                for card in video_cards:
                    video_info = self._extract_video_info_from_container(card, enhanced_info)
                    if video_info and self._is_valid_video_info(video_info):
                        results.append(video_info)
                        debug_print(f"âœ… æå–åˆ°çƒ­æ¦œè§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_info.get('video_code', 'æ— ä»£ç ')})")
            
            # æ–¹æ³•2: å¦‚æœæ²¡æœ‰æ‰¾åˆ°çƒ­æ¦œå®¹å™¨ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            if not results:
                debug_print("ğŸ” æ–¹æ³•1æ— ç»“æœï¼Œå°è¯•é€šç”¨æ–¹æ³•")
                results = self._parse_generic_video_list(soup)
            
            # æ–¹æ³•3: å¦‚æœè¿˜æ˜¯æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨æœç´¢é¡µé¢çš„è§£ææ–¹æ³•
            if not results:
                debug_print("ğŸ” æ–¹æ³•2æ— ç»“æœï¼Œå°è¯•æœç´¢é¡µé¢è§£ææ–¹æ³•")
                results = self._parse_search_page(html_content, f"çƒ­æ¦œ-{category}", enhanced_info, max_results)
            
            debug_print(f"ğŸ¯ æœ€ç»ˆæå–åˆ° {len(results)} ä¸ªçƒ­æ¦œè§†é¢‘ç»“æœ")
            
        except Exception as e:
            debug_print(f"âŒ è§£æçƒ­æ¦œé¡µé¢å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
        
        return results[:max_results]  # æ ¹æ®å‚æ•°é™åˆ¶ç»“æœæ•°é‡
    
    def _extract_video_info_from_container(self, container, extract_enhanced: bool = False) -> Optional[Dict]:
        """ä»è§†é¢‘å®¹å™¨ä¸­æå–ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        try:
            video_info = {
                "url": "",
                "video_code": "",
                "title": "",
                "thumbnail": "",
                "duration": "",
                "publish_date": "",
                "views": ""
            }
            
            # æå–é“¾æ¥ - æ›´å…¨é¢çš„æœç´¢
            link = container.find('a', href=True)
            if not link:
                # å°è¯•åœ¨çˆ¶å…ƒç´ ä¸­æŸ¥æ‰¾
                parent = container.parent
                if parent:
                    link = parent.find('a', href=True)
            
            if link:
                href = link.get('href', '')
                if href.startswith('/'):
                    href = urljoin(self.base_url, href)
                elif not href.startswith('http'):
                    href = urljoin(self.base_url, '/' + href)
                
                if self._is_missav_video_link(href):
                    video_info["url"] = href
                    video_info["video_code"] = self._extract_video_code_from_url(href)
            
            # æå–æ ‡é¢˜ - å¤šç§æ–¹å¼
            title = ""
            # æ–¹å¼1: ä»æ ‡é¢˜æ ‡ç­¾
            title_elem = container.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            if title_elem:
                title = title_elem.get_text(strip=True)
            
            # æ–¹å¼2: ä»titleå±æ€§
            if not title:
                title_attr_elem = container.find(attrs={'title': True})
                if title_attr_elem:
                    title = title_attr_elem.get('title', '').strip()
            
            # æ–¹å¼3: ä»altå±æ€§
            if not title:
                img_with_alt = container.find('img', alt=True)
                if img_with_alt:
                    title = img_with_alt.get('alt', '').strip()
            
            # æ–¹å¼4: ä»é“¾æ¥æ–‡æœ¬
            if not title and link:
                title = link.get_text(strip=True)
            
            video_info["title"] = title
            
            # æå–ç¼©ç•¥å›¾ - å¤šç§æ–¹å¼ï¼Œä¼˜å…ˆè·å–çœŸå®å›¾ç‰‡URL
            img = container.find('img')
            if img:
                # ä¼˜å…ˆçº§ï¼šdata-src > src > data-lazy > data-original
                src = (img.get('data-src') or img.get('src') or 
                      img.get('data-lazy') or img.get('data-original'))
                
                # è¿‡æ»¤æ‰base64å’Œå ä½ç¬¦å›¾ç‰‡
                if src and not src.startswith('data:') and 'placeholder' not in src.lower():
                    if src.startswith('/'):
                        src = urljoin(self.base_url, src)
                    elif not src.startswith('http'):
                        src = urljoin(self.base_url, '/' + src)
                    video_info["thumbnail"] = src
                else:
                    # å°è¯•ä»è§†é¢‘ä»£ç æ„å»ºå°é¢URL
                    if video_info["video_code"]:
                        # MissAVçš„å°é¢å›¾ç‰‡é€šå¸¸åœ¨fourhoi.comåŸŸåä¸‹
                        cover_url = f"https://fourhoi.com/{video_info['video_code']}/cover-n.jpg"
                        video_info["thumbnail"] = cover_url
            
            # æå–æ—¶é•¿
            duration_patterns = [
                r'\b(\d{1,2}:\d{2}:\d{2})\b',  # HH:MM:SS
                r'\b(\d{1,2}:\d{2})\b'        # MM:SS
            ]
            
            container_text = container.get_text()
            for pattern in duration_patterns:
                duration_match = re.search(pattern, container_text)
                if duration_match:
                    video_info["duration"] = duration_match.group(1)
                    break
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_patterns = [
                r'\b(\d{4}-\d{2}-\d{2})\b',
                r'\b(\d{2}/\d{2}/\d{4})\b',
                r'\b(\d{4}/\d{2}/\d{2})\b'
            ]
            
            for pattern in date_patterns:
                date_match = re.search(pattern, container_text)
                if date_match:
                    video_info["publish_date"] = date_match.group(1)
                    break
            
            # å¦‚æœéœ€è¦å¢å¼ºä¿¡æ¯ï¼Œå°è¯•æå–æ›´å¤šè¯¦ç»†ä¿¡æ¯
            if extract_enhanced and video_info["url"]:
                enhanced_info = self._extract_enhanced_info_from_container(container, video_info)
                video_info.update(enhanced_info)
            
            return video_info if video_info["url"] and video_info["video_code"] else None
            
        except Exception as e:
            debug_print(f"âŒ æå–è§†é¢‘å®¹å™¨ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def _extract_enhanced_info_from_container(self, container, basic_info: Dict) -> Dict:
        """ä»å®¹å™¨ä¸­æå–å¢å¼ºä¿¡æ¯"""
        enhanced_info = {}
        
        try:
            container_text = container.get_text()
            
            # æå–æ¼”å‘˜ä¿¡æ¯
            actresses = []
            actress_patterns = [
                r'æ¼”å‘˜[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'å¥³ä¼˜[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'ä¸»æ¼”[ï¼š:]\s*([^ï¼Œ,\n]+)'
            ]
            
            for pattern in actress_patterns:
                matches = re.findall(pattern, container_text)
                for match in matches:
                    actresses.extend([name.strip() for name in re.split(r'[ï¼Œ,ã€]', match) if name.strip()])
            
            if actresses:
                enhanced_info["actresses"] = list(set(actresses))
            
            # æå–ç±»å‹/æ ‡ç­¾
            tags = []
            tag_patterns = [
                r'ç±»å‹[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'æ ‡ç­¾[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'åˆ†ç±»[ï¼š:]\s*([^ï¼Œ,\n]+)'
            ]
            
            for pattern in tag_patterns:
                matches = re.findall(pattern, container_text)
                for match in matches:
                    tags.extend([tag.strip() for tag in re.split(r'[ï¼Œ,ã€]', match) if tag.strip()])
            
            if tags:
                enhanced_info["tags"] = list(set(tags))
            
            # æå–ç³»åˆ—ä¿¡æ¯
            series_patterns = [
                r'ç³»åˆ—[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'ã‚·ãƒªãƒ¼ã‚º[ï¼š:]\s*([^ï¼Œ,\n]+)'
            ]
            
            for pattern in series_patterns:
                match = re.search(pattern, container_text)
                if match:
                    enhanced_info["series"] = match.group(1).strip()
                    break
            
            # æå–å‘è¡Œå•†ä¿¡æ¯
            studio_patterns = [
                r'å‘è¡Œå•†[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'åˆ¶ä½œå•†[ï¼š:]\s*([^ï¼Œ,\n]+)',
                r'ã‚¹ã‚¿ã‚¸ã‚ª[ï¼š:]\s*([^ï¼Œ,\n]+)'
            ]
            
            for pattern in studio_patterns:
                match = re.search(pattern, container_text)
                if match:
                    enhanced_info["studio"] = match.group(1).strip()
                    break
            
            # æå–è§‚çœ‹æ¬¡æ•°
            views_patterns = [
                r'è§‚çœ‹[ï¼š:]\s*(\d+)',
                r'æ’­æ”¾[ï¼š:]\s*(\d+)',
                r'(\d+)\s*æ¬¡è§‚çœ‹'
            ]
            
            for pattern in views_patterns:
                match = re.search(pattern, container_text)
                if match:
                    enhanced_info["view_count"] = match.group(1)
                    break
            
            # æå–è¯„åˆ†ä¿¡æ¯
            rating_patterns = [
                r'è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
                r'â˜…\s*(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*åˆ†'
            ]
            
            for pattern in rating_patterns:
                match = re.search(pattern, container_text)
                if match:
                    enhanced_info["rating"] = match.group(1)
                    break
            
        except Exception as e:
            debug_print(f"âš ï¸ æå–å¢å¼ºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return enhanced_info
    
    def _enrich_video_results(self, results: List[Dict]) -> List[Dict]:
        """ä¸ºè§†é¢‘ç»“æœæ·»åŠ å¢å¼ºä¿¡æ¯"""
        enriched_results = []
        
        for i, video in enumerate(results):
            try:
                debug_print(f"ğŸ“Š è·å–ç¬¬ {i+1}/{len(results)} ä¸ªè§†é¢‘çš„å¢å¼ºä¿¡æ¯: {video.get('video_code', 'æœªçŸ¥')}")
                
                # å°è¯•ä»è§†é¢‘é¡µé¢è·å–æ›´è¯¦ç»†çš„ä¿¡æ¯
                enhanced_video = self._get_enhanced_video_info(video)
                enriched_results.append(enhanced_video)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                if i < len(results) - 1:
                    time.sleep(0.5)
                    
            except Exception as e:
                debug_print(f"âš ï¸ è·å–è§†é¢‘ {video.get('video_code', 'æœªçŸ¥')} çš„å¢å¼ºä¿¡æ¯å¤±è´¥: {str(e)}")
                # å¦‚æœè·å–å¢å¼ºä¿¡æ¯å¤±è´¥ï¼Œä¿ç•™åŸå§‹ä¿¡æ¯
                enriched_results.append(video)
        
        return enriched_results
    
    def _get_enhanced_video_info(self, basic_video_info: Dict) -> Dict:
        """è·å–å•ä¸ªè§†é¢‘çš„å¢å¼ºä¿¡æ¯ - ä½¿ç”¨ä¸GetEnhancedVideoInfoç›¸åŒçš„é€»è¾‘"""
        try:
            video_url = basic_video_info.get("url")
            video_code = basic_video_info.get("video_code")
            
            if not video_url or not video_code:
                return basic_video_info
            
            # æ„å»ºæ­£ç¡®çš„è§†é¢‘é¡µé¢URLï¼Œç¡®ä¿è®¿é—®ä¸­æ–‡é¡µé¢
            # ç§»é™¤URLä¸­çš„ /en éƒ¨åˆ†ä»¥è·å–ä¸­æ–‡å†…å®¹
            if '/en/' in video_url:
                video_url = video_url.replace('/en/', '/')
            elif video_url.endswith('/en'):
                video_url = video_url[:-3]
            
            # å¦‚æœURLåŒ…å« /dmXX/ æ ¼å¼ï¼Œä¿æŒåŸæ ·ï¼Œå› ä¸ºè¿™å¯èƒ½æ˜¯æ­£ç¡®çš„è·¯å¾„
            # åªæœ‰å½“URLæ ¼å¼ä¸æ­£ç¡®æ—¶æ‰ä½¿ç”¨æ ‡å‡†æ ¼å¼
            if not video_url.endswith(video_code) and '/dm' not in video_url:
                video_url = f"{self.base_url}/{video_code}"
            
            # å°è¯•ä½¿ç”¨ä¸GetEnhancedVideoInfoç›¸åŒçš„æ–¹æ³•
            try:
                # å¯¼å…¥å¹¶ä½¿ç”¨ç›¸åŒçš„ä¿¡æ¯æå–å™¨
                from .missav_api import Client
                api_client = Client()
                
                if hasattr(api_client, 'get_enhanced_video_info'):
                    enhanced_result = api_client.get_enhanced_video_info(video_url, use_cache=False)
                    if enhanced_result.get("success"):
                        # æ•°æ®ç›´æ¥åœ¨è¿”å›ç»“æœçš„æ ¹çº§åˆ«ï¼Œä¸åœ¨infoå­—æ®µä¸­
                        enhanced_data = enhanced_result
                        
                        # å°†å¢å¼ºæ•°æ®åˆå¹¶åˆ°åŸºç¡€ä¿¡æ¯ä¸­
                        result = basic_video_info.copy()
                        
                        # æ˜ å°„å­—æ®µåï¼ˆä½¿ç”¨APIå®é™…è¿”å›çš„å­—æ®µåï¼‰
                        field_mapping = {
                            'title': 'title',  # æ·»åŠ æ ‡é¢˜å­—æ®µæ˜ å°„
                            'actresses': 'actresses',
                            'types': 'tags',  # APIä¸­ç±»å‹å­—æ®µå«types
                            'tags': 'labels',  # APIä¸­æ ‡ç­¾å­—æ®µå«tags
                            'publisher': 'studio',  # APIä¸­å‘è¡Œå•†å­—æ®µå«publisher
                            'duration': 'duration',
                            'release_date': 'release_date',
                            'description': 'description',
                            'available_resolutions': 'available_resolutions',
                            'preview_videos': 'preview_videos',
                            'm3u8_url': 'main_m3u8',  # APIä¸­M3U8å­—æ®µå«m3u8_url
                            'main_preview': 'main_preview'
                        }
                        
                        for api_field, result_field in field_mapping.items():
                            if api_field in enhanced_data and enhanced_data[api_field]:
                                result[result_field] = enhanced_data[api_field]
                        
                        # å¤„ç†åˆ†è¾¨ç‡ä¿¡æ¯
                        if 'available_resolutions' in enhanced_data and enhanced_data['available_resolutions']:
                            resolutions = enhanced_data['available_resolutions']
                            if isinstance(resolutions, list):
                                formatted_resolutions = []
                                for res in resolutions:
                                    if isinstance(res, dict) and 'quality' in res and 'resolution' in res:
                                        formatted_resolutions.append({
                                            "quality": res['quality'],
                                            "resolution": res['resolution']
                                        })
                                    elif isinstance(res, str):
                                        formatted_resolutions.append({
                                            "quality": res,
                                            "resolution": "æœªçŸ¥"
                                        })
                                
                                result['available_resolutions'] = formatted_resolutions
                                result['resolution_count'] = len(formatted_resolutions)
                        
                        # ç¡®ä¿å°é¢å›¾ç‰‡URLæ­£ç¡®
                        if enhanced_data.get('main_cover'):
                            result["thumbnail"] = enhanced_data['main_cover']
                        elif not result.get("thumbnail") or result.get("thumbnail").startswith('data:'):
                            result["thumbnail"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
                        
                        return result
            except Exception as api_error:
                debug_print(f"âš ï¸ ä½¿ç”¨APIæ–¹æ³•å¤±è´¥ï¼Œå›é€€åˆ°è‡ªå®šä¹‰æ–¹æ³•: {str(api_error)}")
            
            # å›é€€åˆ°è‡ªå®šä¹‰çš„ä¿¡æ¯æå–æ–¹æ³•
            response = self.session.get(video_url, timeout=15)
            response.raise_for_status()
            
            # è§£æé¡µé¢å†…å®¹
            enhanced_info = self._extract_enhanced_info_from_page(response.text, video_url)
            
            # åˆå¹¶åŸºç¡€ä¿¡æ¯å’Œå¢å¼ºä¿¡æ¯
            result = basic_video_info.copy()
            result.update(enhanced_info)
            
            # ç¡®ä¿å°é¢å›¾ç‰‡URLæ­£ç¡®
            if not result.get("thumbnail") or result.get("thumbnail").startswith('data:'):
                result["thumbnail"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
            
            return result
            
        except Exception as e:
            debug_print(f"âš ï¸ è·å–å¢å¼ºä¿¡æ¯å‡ºé”™: {str(e)}")
            # å³ä½¿è·å–å¢å¼ºä¿¡æ¯å¤±è´¥ï¼Œä¹Ÿè¦ç¡®ä¿å°é¢å›¾ç‰‡URLæ­£ç¡®
            result = basic_video_info.copy()
            if video_code and (not result.get("thumbnail") or result.get("thumbnail").startswith('data:')):
                result["thumbnail"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
            return result
    
    def _extract_enhanced_info_from_page(self, html_content: str, url: str) -> Dict:
        """ä»è§†é¢‘é¡µé¢æå–å¢å¼ºä¿¡æ¯"""
        enhanced_info = {}
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            page_text = soup.get_text()
            
            # æå–ç²¾ç¡®çš„æ—¶é•¿ä¿¡æ¯
            duration_patterns = [
                r'æ—¶é•¿[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})',
                r'æ™‚é•·[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})',
                r'duration[ï¼š:]\s*(\d{1,2}:\d{2}:\d{2})',
                r'(\d{1,2}:\d{2}:\d{2})'
            ]
            
            for pattern in duration_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    enhanced_info["duration"] = match.group(1)
                    enhanced_info["precise_duration"] = match.group(1)
                    break
            
            # æå–æ¼”å‘˜ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰- å¤šç§æ–¹å¼
            actresses_info = []
            
            # æ–¹å¼1: é€šè¿‡é“¾æ¥æŸ¥æ‰¾
            actress_links = soup.find_all('a', href=re.compile(r'/actress/|/actresses/'))
            for link in actress_links:
                actress_name = link.get_text(strip=True)
                actress_url = link.get('href')
                if actress_name and len(actress_name) > 1 and not actress_name.lower() in ['actress', 'actresses']:
                    actresses_info.append({
                        "name": actress_name,
                        "url": urljoin(self.base_url, actress_url) if actress_url.startswith('/') else actress_url
                    })
            
            # æ–¹å¼2: é€šè¿‡æ–‡æœ¬æ¨¡å¼æŸ¥æ‰¾
            if not actresses_info:
                actress_patterns = [
                    r'æ¼”å‘˜[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'å¥³ä¼˜[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'ä¸»æ¼”[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'å‡ºæ¼”[ï¼š:]\s*([^ï¼Œ,\n\r]+)'
                ]
                
                for pattern in actress_patterns:
                    matches = re.findall(pattern, page_text)
                    for match in matches:
                        names = [name.strip() for name in re.split(r'[ï¼Œ,ã€]', match) if name.strip()]
                        for name in names:
                            if len(name) > 1:
                                actresses_info.append({"name": name, "url": ""})
            
            if actresses_info:
                enhanced_info["actresses_with_links"] = actresses_info
                enhanced_info["actresses"] = [actress["name"] for actress in actresses_info]
            
            # æå–ç±»å‹/æ ‡ç­¾ä¿¡æ¯ï¼ˆå¸¦é“¾æ¥ï¼‰- å¤šç§æ–¹å¼
            tags_info = []
            
            # æ–¹å¼1: é€šè¿‡é“¾æ¥æŸ¥æ‰¾
            tag_links = soup.find_all('a', href=re.compile(r'/tag/|/genre/|/genres/'))
            for link in tag_links:
                tag_name = link.get_text(strip=True)
                tag_url = link.get('href')
                if tag_name and len(tag_name) > 1 and not tag_name.lower() in ['tag', 'tags', 'genre', 'genres']:
                    tags_info.append({
                        "name": tag_name,
                        "url": urljoin(self.base_url, tag_url) if tag_url.startswith('/') else tag_url
                    })
            
            # æ–¹å¼2: é€šè¿‡æ–‡æœ¬æ¨¡å¼æŸ¥æ‰¾
            if not tags_info:
                tag_patterns = [
                    r'ç±»å‹[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'æ ‡ç­¾[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'åˆ†ç±»[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'é¡å‹[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'æ¨™ç±¤[ï¼š:]\s*([^ï¼Œ,\n\r]+)'
                ]
                
                for pattern in tag_patterns:
                    matches = re.findall(pattern, page_text)
                    for match in matches:
                        tags = [tag.strip() for tag in re.split(r'[ï¼Œ,ã€]', match) if tag.strip()]
                        for tag in tags:
                            if len(tag) > 1:
                                tags_info.append({"name": tag, "url": ""})
            
            if tags_info:
                enhanced_info["tags_with_links"] = tags_info
                enhanced_info["tags"] = [tag["name"] for tag in tags_info]
            
            # æå–ç³»åˆ—ä¿¡æ¯ - å¤šç§æ–¹å¼
            series_links = soup.find_all('a', href=re.compile(r'/series/'))
            if series_links:
                series_link = series_links[0]
                enhanced_info["series"] = series_link.get_text(strip=True)
                enhanced_info["series_url"] = urljoin(self.base_url, series_link.get('href'))
            else:
                # é€šè¿‡æ–‡æœ¬æ¨¡å¼æŸ¥æ‰¾
                series_patterns = [
                    r'ç³»åˆ—[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'ã‚·ãƒªãƒ¼ã‚º[ï¼š:]\s*([^ï¼Œ,\n\r]+)'
                ]
                for pattern in series_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        enhanced_info["series"] = match.group(1).strip()
                        break
            
            # æå–å‘è¡Œå•†ä¿¡æ¯ - å¤šç§æ–¹å¼
            studio_links = soup.find_all('a', href=re.compile(r'/studio/|/studios/'))
            if studio_links:
                studio_link = studio_links[0]
                enhanced_info["studio"] = studio_link.get_text(strip=True)
                enhanced_info["studio_url"] = urljoin(self.base_url, studio_link.get('href'))
            else:
                # é€šè¿‡æ–‡æœ¬æ¨¡å¼æŸ¥æ‰¾
                studio_patterns = [
                    r'å‘è¡Œå•†[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'åˆ¶ä½œå•†[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'ç™¼è¡Œå•†[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'è£½ä½œå•†[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'ã‚¹ã‚¿ã‚¸ã‚ª[ï¼š:]\s*([^ï¼Œ,\n\r]+)',
                    r'ãƒ¡ãƒ¼ã‚«ãƒ¼[ï¼š:]\s*([^ï¼Œ,\n\r]+)'
                ]
                for pattern in studio_patterns:
                    match = re.search(pattern, page_text)
                    if match:
                        enhanced_info["studio"] = match.group(1).strip()
                        break
            
            # æå–å‘è¡Œæ—¥æœŸ
            date_patterns = [
                r'å‘è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
                r'ç™¼è¡Œæ—¥æœŸ[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
                r'release[ï¼š:]\s*(\d{4}-\d{2}-\d{2})',
                r'(\d{4}-\d{2}-\d{2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    enhanced_info["release_date"] = match.group(1)
                    break
            
            # æå–è§†é¢‘ç®€ä»‹/æè¿° - å¤šç§æ–¹å¼
            description_selectors = [
                'div.description',
                'div.summary',
                'div.plot',
                'p.description',
                'div.content',
                'div.intro',
                '.video-description',
                '.video-summary'
            ]
            
            for selector in description_selectors:
                desc_elem = soup.select_one(selector)
                if desc_elem:
                    description = desc_elem.get_text(strip=True)
                    if description and len(description) > 10:
                        enhanced_info["description"] = description
                        break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æè¿°ï¼Œå°è¯•ä»metaæ ‡ç­¾è·å–
            if not enhanced_info.get("description"):
                meta_desc = soup.find('meta', attrs={'name': 'description'})
                if meta_desc and meta_desc.get('content'):
                    description = meta_desc.get('content').strip()
                    if len(description) > 10:
                        enhanced_info["description"] = description
            
            # æå–æ ‡ç­¾ä¿¡æ¯ï¼ˆæ›´è¯¦ç»†çš„æ–¹å¼ï¼‰
            labels_info = []
            label_links = soup.find_all('a', href=re.compile(r'/labels/'))
            for link in label_links:
                label_name = link.get_text(strip=True)
                label_url = link.get('href')
                if label_name and len(label_name) > 1:
                    labels_info.append({
                        "name": label_name,
                        "url": urljoin(self.base_url, label_url) if label_url.startswith('/') else label_url
                    })
            
            if labels_info:
                enhanced_info["labels_with_links"] = labels_info
                enhanced_info["labels"] = [label["name"] for label in labels_info]
            
            # æå–åˆ¶ä½œå•†ä¿¡æ¯ï¼ˆmakersï¼‰
            maker_links = soup.find_all('a', href=re.compile(r'/makers/'))
            if maker_links:
                maker_link = maker_links[0]
                enhanced_info["maker"] = maker_link.get_text(strip=True)
                enhanced_info["maker_url"] = urljoin(self.base_url, maker_link.get('href'))
            
            # æå–å¯ç”¨åˆ†è¾¨ç‡ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆï¼‰
            resolution_info = self._extract_detailed_resolution_from_page(html_content, soup)
            if resolution_info:
                enhanced_info.update(resolution_info)
            
            # æå–é¢„è§ˆè§†é¢‘ä¿¡æ¯
            preview_info = self._extract_preview_info_from_page(html_content, soup, url)
            if preview_info:
                enhanced_info.update(preview_info)
            
            # æå–M3U8æ’­æ”¾é“¾æ¥
            m3u8_info = self._extract_m3u8_info_from_page(html_content, soup)
            if m3u8_info:
                enhanced_info.update(m3u8_info)
            
            # æå–è§‚çœ‹æ¬¡æ•°å’Œè¯„åˆ†ä¿¡æ¯
            views_patterns = [
                r'è§‚çœ‹[ï¼š:]\s*(\d+)',
                r'æ’­æ”¾[ï¼š:]\s*(\d+)',
                r'(\d+)\s*æ¬¡è§‚çœ‹',
                r'(\d+)\s*views',
                r'è§€çœ‹[ï¼š:]\s*(\d+)'
            ]
            
            for pattern in views_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    enhanced_info["views"] = match.group(1)
                    enhanced_info["view_count"] = match.group(1)
                    break
            
            # æå–è¯„åˆ†ä¿¡æ¯
            rating_patterns = [
                r'è¯„åˆ†[ï¼š:]\s*(\d+\.?\d*)',
                r'è©•åˆ†[ï¼š:]\s*(\d+\.?\d*)',
                r'â˜…\s*(\d+\.?\d*)',
                r'(\d+\.?\d*)\s*åˆ†',
                r'rating[ï¼š:]\s*(\d+\.?\d*)'
            ]
            
            for pattern in rating_patterns:
                match = re.search(pattern, page_text, re.IGNORECASE)
                if match:
                    enhanced_info["rating"] = match.group(1)
                    break
            
            # æå–å°é¢å›¾ç‰‡ä¿¡æ¯ï¼ˆé«˜æ¸…ç‰ˆæœ¬ï¼‰
            cover_imgs = soup.find_all('img', src=re.compile(r'cover|thumb'))
            if cover_imgs:
                main_cover = cover_imgs[0].get('src')
                if main_cover and not main_cover.startswith('data:'):
                    enhanced_info["main_cover"] = urljoin(self.base_url, main_cover) if main_cover.startswith('/') else main_cover
            
            # å°è¯•è·å–é«˜æ¸…å°é¢
            video_code = self._extract_video_code_from_url(url)
            if video_code:
                enhanced_info["cover_image"] = f"https://fourhoi.com/{video_code}/cover-n.jpg"
            
        except Exception as e:
            debug_print(f"âš ï¸ è§£æå¢å¼ºä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return enhanced_info
    
    def _extract_detailed_resolution_from_page(self, html_content: str, soup) -> Dict:
        """ä»é¡µé¢æå–è¯¦ç»†çš„åˆ†è¾¨ç‡ä¿¡æ¯"""
        resolution_info = {}
        
        try:
            # æŸ¥æ‰¾JavaScriptä¸­çš„åˆ†è¾¨ç‡ä¿¡æ¯
            resolution_patterns = [
                r'"(\d{3,4}p)"\s*:\s*"([^"]+)"',  # "1080p": "1920x1080"
                r'(\d{3,4}p)\s*\((\d+x\d+)\)',    # 1080p (1920x1080)
                r'(\d{3,4})x(\d{3,4})',           # 1920x1080
                r'"quality":\s*"(\d{3,4}p)"',     # "quality": "1080p"
            ]
            
            available_resolutions = []
            
            for pattern in resolution_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    if isinstance(match, tuple) and len(match) >= 2:
                        if 'p' in match[0]:
                            quality = match[0]
                            resolution = match[1] if 'x' in match[1] else f"{match[1]}x{match[0].replace('p', '')}"
                        else:
                            quality = f"{match[1]}p"
                            resolution = f"{match[0]}x{match[1]}"
                        
                        available_resolutions.append({
                            "quality": quality,
                            "resolution": resolution
                        })
            
            # æ ‡å‡†åˆ†è¾¨ç‡æ˜ å°„
            standard_resolutions = {
                "360p": "640x360",
                "480p": "854x480", 
                "720p": "1280x720",
                "1080p": "1920x1080",
                "1440p": "2560x1440",
                "2160p": "3840x2160"  # 4K
            }
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“åˆ†è¾¨ç‡ï¼Œä½¿ç”¨æ ‡å‡†æ˜ å°„å’Œé»˜è®¤åˆ†è¾¨ç‡
            if not available_resolutions:
                # æŸ¥æ‰¾æ›´å¤šåˆ†è¾¨ç‡æ¨¡å¼
                quality_matches = re.findall(r'(\d{3,4}p)', html_content, re.IGNORECASE)
                found_qualities = set()
                
                for quality in quality_matches:
                    quality_lower = quality.lower()
                    if quality_lower in standard_resolutions and quality_lower not in found_qualities:
                        available_resolutions.append({
                            "quality": quality,
                            "resolution": standard_resolutions[quality_lower]
                        })
                        found_qualities.add(quality_lower)
                
                # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤çš„å¸¸è§åˆ†è¾¨ç‡
                if not available_resolutions:
                    default_resolutions = [
                        {"quality": "360p", "resolution": "640x360"},
                        {"quality": "480p", "resolution": "854x480"},
                        {"quality": "720p", "resolution": "1280x720"},
                        {"quality": "1080p", "resolution": "1920x1080"}
                    ]
                    available_resolutions = default_resolutions
            
            if available_resolutions:
                # å»é‡å¹¶æ’åº
                unique_resolutions = []
                seen = set()
                for res in available_resolutions:
                    key = res["quality"].lower()
                    if key not in seen:
                        unique_resolutions.append(res)
                        seen.add(key)
                
                # æŒ‰è´¨é‡æ’åº
                quality_order = {"2160p": 0, "1440p": 1, "1080p": 2, "720p": 3, "480p": 4, "360p": 5}
                unique_resolutions.sort(key=lambda x: quality_order.get(x["quality"].lower(), 99))
                
                resolution_info["available_resolutions"] = unique_resolutions
                resolution_info["resolution_count"] = len(unique_resolutions)
                
                if unique_resolutions:
                    resolution_info["best_quality"] = unique_resolutions[0]["quality"]
                    resolution_info["best_resolution"] = unique_resolutions[0]["resolution"]
            
        except Exception as e:
            debug_print(f"âš ï¸ æå–è¯¦ç»†åˆ†è¾¨ç‡ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return resolution_info
    
    def _extract_preview_info_from_page(self, html_content: str, soup, url: str) -> Dict:
        """ä»é¡µé¢æå–é¢„è§ˆè§†é¢‘ä¿¡æ¯"""
        preview_info = {}
        
        try:
            video_code = self._extract_video_code_from_url(url)
            
            # æŸ¥æ‰¾é¢„è§ˆè§†é¢‘é“¾æ¥
            preview_patterns = [
                r'"preview":\s*"([^"]+)"',
                r'preview["\']?\s*:\s*["\']([^"\']+)["\']',
                r'preview\.mp4["\']?\s*:\s*["\']([^"\']+)["\']',
                r'(https?://[^"\s]+/preview\.mp4)',
                r'(https?://fourhoi\.com/[^/]+/preview\.mp4)'
            ]
            
            preview_urls = []
            for pattern in preview_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                preview_urls.extend(matches)
            
            # æ„å»ºæ ‡å‡†é¢„è§ˆURL
            if video_code:
                standard_preview = f"https://fourhoi.com/{video_code}/preview.mp4"
                preview_urls.append(standard_preview)
            
            if preview_urls:
                # å»é‡
                unique_previews = list(set(preview_urls))
                preview_info["preview_videos"] = unique_previews
                preview_info["preview_count"] = len(unique_previews)
                preview_info["main_preview"] = unique_previews[0]
            
        except Exception as e:
            debug_print(f"âš ï¸ æå–é¢„è§ˆè§†é¢‘ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return preview_info
    
    def _extract_m3u8_info_from_page(self, html_content: str, soup) -> Dict:
        """ä»é¡µé¢æå–M3U8æ’­æ”¾é“¾æ¥ä¿¡æ¯"""
        m3u8_info = {}
        
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥æŸ¥æ‰¾M3U8é“¾æ¥
            m3u8_patterns = [
                r'"(https?://[^"]+\.m3u8[^"]*)"',
                r"'(https?://[^']+\.m3u8[^']*)'",
                r'src:\s*["\']([^"\']+\.m3u8[^"\']*)["\']',
                r'playlist["\']?\s*:\s*["\']([^"\']+\.m3u8[^"\']*)["\']',
                r'(https?://surrit\.com/[^/]+/playlist\.m3u8)',
                r'(https?://[^"\s]+/playlist\.m3u8)'
            ]
            
            m3u8_urls = []
            for pattern in m3u8_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                m3u8_urls.extend(matches)
            
            # å¦‚æœç›´æ¥æœç´¢æ²¡æœ‰ç»“æœï¼Œå°è¯•è§£ç æ··æ·†çš„JavaScript
            if not m3u8_urls:
                m3u8_urls = self._decode_obfuscated_js_for_m3u8(html_content)
            
            if m3u8_urls:
                # å»é‡
                unique_m3u8 = list(set(m3u8_urls))
                m3u8_info["m3u8_playlists"] = unique_m3u8
                m3u8_info["m3u8_count"] = len(unique_m3u8)
                m3u8_info["main_m3u8"] = unique_m3u8[0]
            
        except Exception as e:
            debug_print(f"âš ï¸ æå–M3U8ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        
        return m3u8_info
    
    def _decode_obfuscated_js_for_m3u8(self, html_content: str) -> List[str]:
        """è§£ç æ··æ·†çš„JavaScriptæ¥æŸ¥æ‰¾M3U8é“¾æ¥"""
        m3u8_urls = []
        
        try:
            # æŸ¥æ‰¾æ··æ·†çš„JavaScriptä»£ç 
            obfuscated_patterns = [
                r'eval\(function\(p,a,c,k,e,d\)\{[^}]+\}[^)]+\)',
                r'eval\([^)]+\)'
            ]
            
            for pattern in obfuscated_patterns:
                matches = re.findall(pattern, html_content, re.DOTALL)
                for match in matches:
                    try:
                        # å°è¯•ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢è§£ç 
                        decoded = self._simple_js_decode(match)
                        if decoded:
                            # åœ¨è§£ç åçš„å†…å®¹ä¸­æŸ¥æ‰¾M3U8é“¾æ¥
                            m3u8_patterns = [
                                r'(https?://[^"\s\']+\.m3u8[^"\s\']*)',
                                r'(https?://surrit\.com/[^/\s"\']+/playlist\.m3u8)',
                            ]
                            
                            for m3u8_pattern in m3u8_patterns:
                                found_urls = re.findall(m3u8_pattern, decoded, re.IGNORECASE)
                                m3u8_urls.extend(found_urls)
                    except:
                        continue
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾base64ç¼–ç çš„å†…å®¹
            if not m3u8_urls:
                m3u8_urls = self._find_base64_m3u8(html_content)
            
        except Exception as e:
            debug_print(f"âš ï¸ è§£ç æ··æ·†JavaScriptæ—¶å‡ºé”™: {str(e)}")
        
        return m3u8_urls
    
    def _simple_js_decode(self, obfuscated_code: str) -> str:
        """ç®€å•çš„JavaScriptè§£ç """
        try:
            # æŸ¥æ‰¾å­—ç¬¦ä¸²æ•°ç»„
            array_match = re.search(r'\[([^\]]+)\]', obfuscated_code)
            if not array_match:
                return ""
            
            # æå–å­—ç¬¦ä¸²æ•°ç»„
            array_content = array_match.group(1)
            strings = []
            
            # è§£æå­—ç¬¦ä¸²æ•°ç»„
            string_matches = re.findall(r'["\']([^"\']*)["\']', array_content)
            strings.extend(string_matches)
            
            # å°è¯•é‡å»ºåŸå§‹å­—ç¬¦ä¸²
            decoded_parts = []
            for s in strings:
                if 'surrit' in s or 'm3u8' in s or 'playlist' in s:
                    decoded_parts.append(s)
            
            return ' '.join(decoded_parts)
            
        except Exception as e:
            debug_print(f"âš ï¸ ç®€å•JSè§£ç å¤±è´¥: {str(e)}")
            return ""
    
    def _find_base64_m3u8(self, html_content: str) -> List[str]:
        """æŸ¥æ‰¾å¯èƒ½çš„base64ç¼–ç çš„M3U8é“¾æ¥"""
        m3u8_urls = []
        
        try:
            import base64
            
            # æŸ¥æ‰¾å¯èƒ½çš„base64ç¼–ç å­—ç¬¦ä¸²
            base64_patterns = [
                r'[A-Za-z0-9+/]{20,}={0,2}',  # åŸºæœ¬base64æ¨¡å¼
            ]
            
            for pattern in base64_patterns:
                matches = re.findall(pattern, html_content)
                for match in matches:
                    try:
                        decoded = base64.b64decode(match).decode('utf-8', errors='ignore')
                        if 'm3u8' in decoded.lower() or 'surrit' in decoded.lower():
                            # åœ¨è§£ç å†…å®¹ä¸­æŸ¥æ‰¾URL
                            url_matches = re.findall(r'https?://[^\s"\'<>]+', decoded)
                            for url in url_matches:
                                if 'm3u8' in url.lower():
                                    m3u8_urls.append(url)
                    except:
                        continue
            
        except Exception as e:
            debug_print(f"âš ï¸ Base64è§£ç å¤±è´¥: {str(e)}")
        
        return m3u8_urls
    
    def _extract_video_info_from_link_enhanced(self, link) -> Optional[Dict]:
        """ä»é“¾æ¥å…ƒç´ ä¸­æå–ä¿¡æ¯ - å¢å¼ºç‰ˆ"""
        try:
            href = link.get('href', '')
            if href.startswith('/'):
                href = urljoin(self.base_url, href)
            elif not href.startswith('http'):
                href = urljoin(self.base_url, '/' + href)
            
            video_info = {
                "url": href,
                "video_code": self._extract_video_code_from_url(href),
                "title": "",
                "thumbnail": "",
                "duration": "",
                "publish_date": "",
                "views": ""
            }
            
            # æå–æ ‡é¢˜ - å¢å¼ºç‰ˆ
            title = link.get_text(strip=True) or link.get('title', '') or link.get('alt', '')
            
            if not title:
                # å°è¯•ä»å­å…ƒç´ è·å–
                title_elem = link.find(['span', 'div', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
                if title_elem:
                    title = title_elem.get_text(strip=True)
            
            if not title:
                # å°è¯•ä»çˆ¶å…ƒç´ è·å–æ ‡é¢˜
                parent = link.parent
                if parent:
                    # æŸ¥æ‰¾æ ‡é¢˜ç›¸å…³çš„å…ƒç´ 
                    title_selectors = [
                        '.title', '.video-title', '.name', '.video-name',
                        '[class*="title"]', '[class*="name"]'
                    ]
                    for selector in title_selectors:
                        title_elem = parent.select_one(selector)
                        if title_elem:
                            title = title_elem.get_text(strip=True)
                            break
                    
                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»å…„å¼Ÿå…ƒç´ è·å–
                    if not title:
                        for sibling in parent.find_all(['div', 'span', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                            sibling_text = sibling.get_text(strip=True)
                            if sibling_text and len(sibling_text) > 10:  # æ ‡é¢˜é€šå¸¸æ¯”è¾ƒé•¿
                                title = sibling_text
                                break
            
            video_info["title"] = title
            
            # å°è¯•ä»çˆ¶å…ƒç´ æˆ–å…„å¼Ÿå…ƒç´ ä¸­æå–æ›´å¤šä¿¡æ¯
            parent = link.parent
            if parent:
                # æŸ¥æ‰¾ç¼©ç•¥å›¾
                img = parent.find('img')
                if img:
                    src = (img.get('src') or img.get('data-src') or 
                          img.get('data-lazy') or img.get('data-original'))
                    if src:
                        if src.startswith('/'):
                            src = urljoin(self.base_url, src)
                        video_info["thumbnail"] = src
                
                # æŸ¥æ‰¾æ—¶é•¿å’Œæ—¥æœŸ
                parent_text = parent.get_text()
                duration_match = re.search(r'\b(\d{1,2}:\d{2}(?::\d{2})?)\b', parent_text)
                if duration_match:
                    video_info["duration"] = duration_match.group(1)
                
                date_match = re.search(r'\b(\d{4}-\d{2}-\d{2})\b', parent_text)
                if date_match:
                    video_info["publish_date"] = date_match.group(1)
            
            return video_info
            
        except Exception as e:
            debug_print(f"âŒ æå–é“¾æ¥ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def _extract_videos_with_regex(self, html_content: str) -> List[Dict]:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå–è§†é¢‘ä¿¡æ¯"""
        results = []
        
        try:
            # åŒ¹é…MissAVè§†é¢‘é“¾æ¥çš„æ­£åˆ™è¡¨è¾¾å¼
            video_link_patterns = [
                r'href="([^"]*(?:missav\.ws|missav\.com)[^"]*\/[a-zA-Z0-9\-]+)"',
                r'href="(\/[a-zA-Z0-9\-]+)"(?=.*video|.*thumbnail)',
                r'<a[^>]*href="([^"]*\/[a-zA-Z]{2,6}-\d{2,4}[^"]*)"'
            ]
            
            for pattern in video_link_patterns:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    url = match
                    if url.startswith('/'):
                        url = urljoin(self.base_url, url)
                    
                    if self._is_missav_video_link(url):
                        video_code = self._extract_video_code_from_url(url)
                        if video_code and len(video_code) > 2:
                            # å°è¯•æå–æ ‡é¢˜
                            title_pattern = rf'<a[^>]*href="{re.escape(match)}"[^>]*>([^<]+)</a>'
                            title_match = re.search(title_pattern, html_content, re.IGNORECASE)
                            title = title_match.group(1).strip() if title_match else video_code
                            
                            results.append({
                                "url": url,
                                "video_code": video_code,
                                "title": title,
                                "thumbnail": "",
                                "duration": "",
                                "publish_date": "",
                                "views": ""
                            })
                
                if results:
                    break  # å¦‚æœæ‰¾åˆ°ç»“æœå°±åœæ­¢å°è¯•å…¶ä»–æ¨¡å¼
            
        except Exception as e:
            debug_print(f"âŒ æ­£åˆ™è¡¨è¾¾å¼æå–å‡ºé”™: {str(e)}")
        
        return results
    
    def _extract_videos_from_javascript(self, html_content: str) -> List[Dict]:
        """ä»JavaScriptä»£ç ä¸­æå–è§†é¢‘ä¿¡æ¯"""
        results = []
        
        try:
            # æŸ¥æ‰¾JavaScriptä¸­çš„è§†é¢‘æ•°æ®
            js_patterns = [
                r'videos\s*:\s*(\[.*?\])',
                r'videoList\s*:\s*(\[.*?\])',
                r'data\s*:\s*(\[.*?\])',
                r'items\s*:\s*(\[.*?\])'
            ]
            
            for pattern in js_patterns:
                matches = re.findall(pattern, html_content, re.DOTALL)
                for match in matches:
                    try:
                        import json
                        video_data = json.loads(match)
                        if isinstance(video_data, list):
                            for item in video_data:
                                if isinstance(item, dict) and 'url' in item:
                                    results.append({
                                        "url": item.get('url', ''),
                                        "video_code": item.get('code', ''),
                                        "title": item.get('title', ''),
                                        "thumbnail": item.get('thumbnail', ''),
                                        "duration": item.get('duration', ''),
                                        "publish_date": item.get('date', ''),
                                        "views": item.get('views', '')
                                    })
                    except:
                        continue
                
                if results:
                    break
            
        except Exception as e:
            debug_print(f"âŒ JavaScriptæå–å‡ºé”™: {str(e)}")
        
        return results
    
    def _extract_video_info_from_link(self, link) -> Optional[Dict]:
        """ä»é“¾æ¥å…ƒç´ ä¸­æå–ä¿¡æ¯"""
        try:
            href = link.get('href', '')
            if href.startswith('/'):
                href = urljoin(self.base_url, href)
            
            video_info = {
                "url": href,
                "video_code": self._extract_video_code_from_url(href),
                "title": link.get_text(strip=True) or link.get('title', ''),
                "thumbnail": "",
                "duration": "",
                "publish_date": "",
                "views": ""
            }
            
            # å°è¯•ä»çˆ¶å…ƒç´ ä¸­æå–æ›´å¤šä¿¡æ¯
            parent = link.parent
            if parent:
                img = parent.find('img')
                if img:
                    src = img.get('src') or img.get('data-src')
                    if src and src.startswith('/'):
                        src = urljoin(self.base_url, src)
                    video_info["thumbnail"] = src or ""
            
            return video_info
            
        except Exception as e:
            debug_print(f"æå–é“¾æ¥ä¿¡æ¯å‡ºé”™: {str(e)}")
            return None
    
    def _parse_generic_video_list(self, soup) -> List[Dict]:
        """é€šç”¨çš„è§†é¢‘åˆ—è¡¨è§£ææ–¹æ³•"""
        results = []
        seen_codes = set()  # ç”¨äºå»é‡
        
        try:
            # å¯»æ‰¾æ‰€æœ‰å¯èƒ½çš„è§†é¢‘é“¾æ¥
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '')
                if self._is_missav_video_link(href):
                    video_info = self._extract_video_info_from_link_enhanced(link)
                    if video_info and self._is_valid_video_info(video_info):
                        # å»é‡ï¼šåŸºäºè§†é¢‘ä»£ç 
                        video_code = video_info.get('video_code', '')
                        if video_code and video_code not in seen_codes:
                            results.append(video_info)
                            seen_codes.add(video_code)
                            debug_print(f"âœ… æå–åˆ°å”¯ä¸€è§†é¢‘: {video_info.get('title', 'æ— æ ‡é¢˜')} ({video_code})")
                        elif video_code in seen_codes:
                            debug_print(f"âš ï¸ è·³è¿‡é‡å¤è§†é¢‘: {video_code}")
            
        except Exception as e:
            debug_print(f"é€šç”¨è§£æå‡ºé”™: {str(e)}")
        
        return results
    
    def _is_missav_video_link(self, href: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯MissAVè§†é¢‘é“¾æ¥ - å¢å¼ºç‰ˆ"""
        if not href:
            return False
        
        # æ’é™¤æ˜æ˜¾ä¸æ˜¯è§†é¢‘çš„é“¾æ¥
        exclude_patterns = [
            '/search/', '/category/', '/tag/', '/actress/', '/actresses/',
            '/studio/', '/studios/', '/series/', '/page/', '/login',
            '/register', '/contact', '/about', '/api/', '/admin/',
            '/fonts/', '/css/', '/js/', '/images/', '/img/',
            '.css', '.js', '.png', '.jpg', '.gif', '.ico', 
            '.woff', '.woff2', '.ttf', '.svg',
            '/genres/', '/uncensored-leak/', '/chinese-subtitle/',
            '/monthly-hot/', '/weekly-hot/', '/daily-hot/',
            '/new/', '/popular/', '/trending/', '/dm22/', '/dm54/',
            '/dm4416/', '/dm621/', '/dm291/', '/dm169/', '/dm257/',
            '?sort=', '?filters=', '?page=', '#'
        ]
        
        for pattern in exclude_patterns:
            if pattern in href:
                return False
        
        # é¢å¤–æ£€æŸ¥ï¼šæ’é™¤ä»¥è¿™äº›æ— æ•ˆä»£ç ç»“å°¾çš„é“¾æ¥
        invalid_endings = [
            '/uncensored-leak', '/chinese-subtitle', '/english-subtitle',
            '/today-hot', '/weekly-hot', '/monthly-hot', '/new', '/popular',
            '/trending', '/search', '/genres', '/category'
        ]
        
        href_clean = href.split('?')[0].split('#')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        for ending in invalid_endings:
            if href_clean.endswith(ending):
                return False
        
        # MissAVè§†é¢‘é“¾æ¥çš„ç‰¹å¾æ¨¡å¼
        video_patterns = [
            # æ ‡å‡†æ ¼å¼ï¼šå­—æ¯-æ•°å­— (å¦‚ /SSIS-950, /OFJE-505)
            r'/[A-Z]{2,6}-\d{2,4}$',
            # å¸¦åç¼€çš„æ ¼å¼ (å¦‚ /SSIS-950-uncensored)
            r'/[A-Z]{2,6}-\d{2,4}-[a-z-]+$',
            # æ•°å­—å¼€å¤´çš„æ ¼å¼ (å¦‚ /259LUXU-1234)
            r'/\d{2,4}[A-Z]{2,6}-\d{2,4}$',
            # ç‰¹æ®Šæ ¼å¼ (å¦‚ /FC2-PPV-1234567)
            r'/FC2-PPV-\d{6,8}$',
            # å…¶ä»–å¸¸è§æ ¼å¼
            r'/[A-Z]{1,4}\d{2,4}$',  # å¦‚ /ABP123
            # å°å†™æ ¼å¼
            r'/[a-z]{2,6}-\d{2,4}$',
            # æ··åˆæ ¼å¼
            r'/[A-Za-z0-9]{3,10}-[A-Za-z0-9]{2,6}$'
        ]
        
        # æå–URLè·¯å¾„éƒ¨åˆ†è¿›è¡ŒåŒ¹é…
        url_path = href.split('?')[0].split('#')[0]  # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…è§†é¢‘ä»£ç æ¨¡å¼
        for pattern in video_patterns:
            if re.search(pattern, url_path, re.IGNORECASE):
                return True
        
        return False
    
    def _extract_video_code_from_url(self, url: str) -> str:
        """ä»URLä¸­æå–è§†é¢‘ä»£ç """
        try:
            path = urlparse(url).path
            code = path.split('/')[-1]
            return code if code else ""
        except:
            return ""
    
    def _is_valid_video_info(self, video_info: Dict) -> bool:
        """éªŒè¯è§†é¢‘ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ - å¢å¼ºç‰ˆ"""
        if not video_info:
            return False
        
        url = video_info.get("url", "")
        video_code = video_info.get("video_code", "")
        title = video_info.get("title", "")
        
        # åŸºæœ¬éªŒè¯
        if not url or not video_code:
            return False
        
        # è§†é¢‘ä»£ç é•¿åº¦éªŒè¯
        if len(video_code) < 2:
            return False
        
        # URLæœ‰æ•ˆæ€§éªŒè¯
        if not self._is_missav_video_link(url):
            return False
        
        # æ’é™¤æ˜æ˜¾çš„é”™è¯¯ç»“æœ
        invalid_codes = [
            'search', 'page', 'sort', 'filter', 'genres', 'category',
            'english-subtitle', 'chinese-subtitle', 'uncensored-leak',
            'today-hot', 'weekly-hot', 'monthly-hot', 'dm22', 'dm54', 'dm4416'
        ]
        
        if video_code.lower() in invalid_codes:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯URLç¼–ç çš„å…³é”®è¯ï¼ˆå¦‚%E6%8A%A4%E5%A3%ABï¼‰
        if '%' in video_code and len(video_code) > 10:
            return False
        
        return True


def test_unified_search():
    """æµ‹è¯•ç»Ÿä¸€æœç´¢æ¨¡å—"""
    debug_print("ğŸ§ª æµ‹è¯•ç»Ÿä¸€æœç´¢æ¨¡å—")
    debug_print("=" * 50)
    
    search_module = UnifiedSearchModule()
    
    # æµ‹è¯•1: åŸºç¡€æœç´¢
    debug_print("\n--- æµ‹è¯•1: åŸºç¡€æœç´¢ ---")
    result = search_module.search_with_filters("SSIS", page=1, max_results=5)
    if result["success"]:
        debug_print(f"âœ… åŸºç¡€æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {result['total_count']} ä¸ªç»“æœ")
        for i, video in enumerate(result["results"][:3]):
            debug_print(f"  {i+1}. {video['title']} ({video['video_code']})")
    else:
        debug_print(f"âŒ åŸºç¡€æœç´¢å¤±è´¥: {result['error']}")
    
    # æµ‹è¯•2: å¸¦æ’åºçš„æœç´¢
    debug_print("\n--- æµ‹è¯•2: å¸¦æ’åºçš„æœç´¢ ---")
    result = search_module.search_with_filters("OFJE", page=1, sort="views", max_results=5)
    if result["success"]:
        debug_print(f"âœ… æ’åºæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {result['total_count']} ä¸ªç»“æœ")
        debug_print(f"   æ’åºæ–¹å¼: {result['sort_name']}")
        for i, video in enumerate(result["results"][:3]):
            debug_print(f"  {i+1}. {video['title']} ({video['video_code']})")
    else:
        debug_print(f"âŒ æ’åºæœç´¢å¤±è´¥: {result['error']}")
    
    # æµ‹è¯•3: å¸¦è¿‡æ»¤å™¨çš„æœç´¢
    debug_print("\n--- æµ‹è¯•3: å¸¦è¿‡æ»¤å™¨çš„æœç´¢ ---")
    result = search_module.search_with_filters("STARS", page=1, filter_type="chinese_subtitle", max_results=5)
    if result["success"]:
        debug_print(f"âœ… è¿‡æ»¤å™¨æœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {result['total_count']} ä¸ªç»“æœ")
        debug_print(f"   è¿‡æ»¤å™¨: {result['filter_name']}")
        for i, video in enumerate(result["results"][:3]):
            debug_print(f"  {i+1}. {video['title']} ({video['video_code']})")
    else:
        debug_print(f"âŒ è¿‡æ»¤å™¨æœç´¢å¤±è´¥: {result['error']}")
    
    # æµ‹è¯•4: æ’åº+è¿‡æ»¤å™¨ç»„åˆ
    debug_print("\n--- æµ‹è¯•4: æ’åº+è¿‡æ»¤å™¨ç»„åˆ ---")
    result = search_module.search_with_filters(
        "High School Girl", page=1, sort="today_views", 
        filter_type="individual", max_results=5
    )
    if result["success"]:
        debug_print(f"âœ… ç»„åˆæœç´¢æˆåŠŸï¼Œæ‰¾åˆ° {result['total_count']} ä¸ªç»“æœ")
        debug_print(f"   æ’åºæ–¹å¼: {result['sort_name']}")
        debug_print(f"   è¿‡æ»¤å™¨: {result['filter_name']}")
        for i, video in enumerate(result["results"][:3]):
            debug_print(f"  {i+1}. {video['title']} ({video['video_code']})")
    else:
        debug_print(f"âŒ ç»„åˆæœç´¢å¤±è´¥: {result['error']}")
    
    # æµ‹è¯•5: çƒ­æ¦œåŠŸèƒ½
    debug_print("\n--- æµ‹è¯•5: çƒ­æ¦œåŠŸèƒ½ ---")
    result = search_module.get_hot_videos_with_filters("daily", page=1, sort="views")
    if result["success"]:
        debug_print(f"âœ… çƒ­æ¦œè·å–æˆåŠŸï¼Œæ‰¾åˆ° {result['total_count']} ä¸ªç»“æœ")
        debug_print(f"   æ•°æ®æº: {result['source']}")
        for i, video in enumerate(result["results"][:3]):
            debug_print(f"  {i+1}. {video['title']} ({video['video_code']})")
    else:
        debug_print(f"âŒ çƒ­æ¦œè·å–å¤±è´¥: {result['error']}")
    
    debug_print("\nâœ… ç»Ÿä¸€æœç´¢æ¨¡å—æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    test_unified_search()