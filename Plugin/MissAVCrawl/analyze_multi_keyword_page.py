#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æå¤šå…³é”®è¯æœç´¢é¡µé¢çš„HTMLç»“æ„
"""

import sys
import requests
from pathlib import Path
from bs4 import BeautifulSoup

# ç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®å†…çš„æ¨¡å—
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from missav_api_core.unified_search_module import UnifiedSearchModule

def analyze_page_structure():
    """åˆ†æé¡µé¢ç»“æ„å·®å¼‚"""
    print("ğŸ” åˆ†æå¤šå…³é”®è¯æœç´¢é¡µé¢ç»“æ„")
    print("=" * 80)
    
    search_module = UnifiedSearchModule()
    
    # å¯¹æ¯”ä¸¤ä¸ªURL
    urls = {
        "å•å…³é”®è¯": "https://missav.ws/search/ä¸‰ä¸Šæ‚ äºš",
        "å¤šå…³é”®è¯": "https://missav.ws/search/ä¸‰ä¸Šæ‚ äºš+é‡‘å‘+å·¨ä¹³+å£äº¤"
    }
    
    for name, url in urls.items():
        print(f"\nğŸ“‹ åˆ†æ {name} æœç´¢é¡µé¢")
        print(f"URL: {url}")
        print("-" * 60)
        
        try:
            response = search_module.session.get(url, timeout=30)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            print(f"é¡µé¢é•¿åº¦: {len(response.text)}")
            print(f"é¡µé¢æ ‡é¢˜: {soup.title.string if soup.title else 'æ— æ ‡é¢˜'}")
            
            # åˆ†æé¡µé¢ç»“æ„
            print("\nğŸ” é¡µé¢ç»“æ„åˆ†æ:")
            
            # 1. æŸ¥æ‰¾è§†é¢‘å®¹å™¨
            video_containers = soup.find_all(['div'], class_=lambda x: x and any(
                keyword in x.lower() for keyword in ['thumbnail', 'video', 'item', 'card', 'grid']
            ))
            print(f"  è§†é¢‘å®¹å™¨æ•°é‡: {len(video_containers)}")
            
            # 2. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            all_links = soup.find_all('a', href=True)
            video_links = [link for link in all_links if search_module._is_missav_video_link(link.get('href', ''))]
            print(f"  æ€»é“¾æ¥æ•°é‡: {len(all_links)}")
            print(f"  è§†é¢‘é“¾æ¥æ•°é‡: {len(video_links)}")
            
            # 3. æŸ¥æ‰¾ç‰¹å®šçš„classæˆ–id
            common_selectors = [
                '.video-list', '.video-grid', '.search-results', '.content',
                '#video-list', '#search-results', '#content',
                '[class*="video"]', '[class*="item"]', '[class*="card"]'
            ]
            
            for selector in common_selectors:
                elements = soup.select(selector)
                if elements:
                    print(f"  æ‰¾åˆ°é€‰æ‹©å™¨ '{selector}': {len(elements)} ä¸ªå…ƒç´ ")
            
            # 4. æŸ¥æ‰¾å¯èƒ½çš„é”™è¯¯ä¿¡æ¯
            error_indicators = ['no results', 'æ²¡æœ‰ç»“æœ', 'æœªæ‰¾åˆ°', 'not found', 'empty']
            page_text = soup.get_text().lower()
            for indicator in error_indicators:
                if indicator in page_text:
                    print(f"  âš ï¸ å‘ç°å¯èƒ½çš„é”™è¯¯æŒ‡ç¤º: '{indicator}'")
            
            # 5. æŸ¥æ‰¾JavaScriptä¸­çš„æ•°æ®
            scripts = soup.find_all('script')
            js_with_data = []
            for script in scripts:
                if script.string and any(keyword in script.string for keyword in ['video', 'data', 'results']):
                    js_with_data.append(script)
            print(f"  åŒ…å«æ•°æ®çš„JSè„šæœ¬: {len(js_with_data)} ä¸ª")
            
            # 6. æ˜¾ç¤ºå‰å‡ ä¸ªè§†é¢‘é“¾æ¥ï¼ˆå¦‚æœæœ‰ï¼‰
            if video_links:
                print(f"\n  å‰3ä¸ªè§†é¢‘é“¾æ¥:")
                for i, link in enumerate(video_links[:3], 1):
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    print(f"    {i}. {href} - {text[:50]}...")
            
            # 7. æ£€æŸ¥æ˜¯å¦æœ‰åˆ†é¡µä¿¡æ¯
            pagination = soup.find_all(['div', 'nav'], class_=lambda x: x and 'pag' in x.lower())
            if pagination:
                print(f"  åˆ†é¡µå…ƒç´ : {len(pagination)} ä¸ª")
            
        except Exception as e:
            print(f"âŒ åˆ†æå¤±è´¥: {str(e)}")
    
    print(f"\nğŸ” åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    analyze_page_structure()