#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„ç½‘ç»œæ ¸å¿ƒ - è§£å†³403åçˆ¬è™«é—®é¢˜
"""

import time
import random
import httpx
import requests
from typing import Optional, Dict, Any
from pathlib import Path
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

class EnhancedNetworkCore:
    """å¢å¼ºçš„ç½‘ç»œæ ¸å¿ƒï¼Œä¸“é—¨å¤„ç†åçˆ¬è™«"""
    
    def __init__(self):
        self.session = None
        self.last_request_time = 0
        self.min_delay = 2  # æœ€å°å»¶è¿Ÿ2ç§’
        self.max_delay = 5  # æœ€å¤§å»¶è¿Ÿ5ç§’
        
        # å¤šä¸ªUser-Agentè½®æ¢
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0'
        ]
        
        # å¤šä¸ªReferer
        self.referers = [
            'https://www.google.com/',
            'https://www.bing.com/',
            'https://missav.ws/',
            'https://www.missav.ws/',
            'https://missav.com/'
        ]
    
    def get_random_headers(self) -> Dict[str, str]:
        """è·å–éšæœºåŒ–çš„è¯·æ±‚å¤´"""
        return {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
            'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"macOS"',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1',
            'User-Agent': random.choice(self.user_agents),
            'Referer': random.choice(self.referers)
        }
    
    def wait_between_requests(self):
        """è¯·æ±‚é—´å»¶è¿Ÿ"""
        current_time = time.time()
        elapsed = current_time - self.last_request_time
        
        if elapsed < self.min_delay:
            delay = random.uniform(self.min_delay - elapsed, self.max_delay - elapsed)
            if delay > 0:
                print(f"â±ï¸ ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
        
        self.last_request_time = time.time()
    
    def fetch_with_requests(self, url: str, max_retries: int = 3) -> Optional[str]:
        """ä½¿ç”¨requestsåº“è·å–å†…å®¹"""
        for attempt in range(max_retries):
            try:
                self.wait_between_requests()
                
                headers = self.get_random_headers()
                print(f"ğŸ”„ å°è¯• {attempt + 1}/{max_retries} - User-Agent: {headers['User-Agent'][:50]}...")
                
                # ä½¿ç”¨sessionä¿æŒè¿æ¥
                if not hasattr(self, 'requests_session'):
                    self.requests_session = requests.Session()
                
                response = self.requests_session.get(
                    url,
                    headers=headers,
                    timeout=30,
                    allow_redirects=True,
                    verify=True
                )
                
                print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
                
                if response.status_code == 200:
                    content = response.text
                    print(f"âœ… æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                    return content
                elif response.status_code == 403:
                    print(f"âŒ 403é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥...")
                    # å¢åŠ å»¶è¿Ÿ
                    time.sleep(random.uniform(3, 8))
                else:
                    print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                    
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    delay = random.uniform(5, 10)
                    print(f"â±ï¸ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
        
        return None
    
    def fetch_with_httpx(self, url: str, max_retries: int = 3) -> Optional[str]:
        """ä½¿ç”¨httpxåº“è·å–å†…å®¹"""
        for attempt in range(max_retries):
            try:
                self.wait_between_requests()
                
                headers = self.get_random_headers()
                print(f"ğŸ”„ HTTPXå°è¯• {attempt + 1}/{max_retries}")
                
                with httpx.Client(
                    headers=headers,
                    timeout=30,
                    follow_redirects=True,
                    verify=True
                ) as client:
                    response = client.get(url)
                    
                    print(f"ğŸ“Š çŠ¶æ€ç : {response.status_code}")
                    
                    if response.status_code == 200:
                        content = response.text
                        print(f"âœ… æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                        return content
                    elif response.status_code == 403:
                        print(f"âŒ 403é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥...")
                        time.sleep(random.uniform(3, 8))
                    else:
                        print(f"âŒ HTTPé”™è¯¯: {response.status_code}")
                        
            except Exception as e:
                print(f"âŒ HTTPXè¯·æ±‚å¤±è´¥: {str(e)}")
                if attempt < max_retries - 1:
                    delay = random.uniform(5, 10)
                    print(f"â±ï¸ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
        
        return None
    
    def fetch_with_curl_simulation(self, url: str) -> Optional[str]:
        """æ¨¡æ‹Ÿcurlè¯·æ±‚"""
        try:
            import subprocess
            import json
            
            headers = self.get_random_headers()
            
            # æ„å»ºcurlå‘½ä»¤
            curl_cmd = [
                'curl', '-s', '-L',  # silent, follow redirects
                '--max-time', '30',
                '--user-agent', headers['User-Agent'],
                '--referer', headers['Referer'],
                '-H', f"Accept: {headers['Accept']}",
                '-H', f"Accept-Language: {headers['Accept-Language']}",
                '-H', f"Accept-Encoding: {headers['Accept-Encoding']}",
                url
            ]
            
            print(f"ğŸ”§ å°è¯•curlæ¨¡æ‹Ÿè¯·æ±‚...")
            
            result = subprocess.run(
                curl_cmd,
                capture_output=True,
                text=True,
                timeout=35
            )
            
            if result.returncode == 0 and result.stdout:
                content = result.stdout
                print(f"âœ… curlæˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(content)}")
                return content
            else:
                print(f"âŒ curlå¤±è´¥: {result.stderr}")
                
        except Exception as e:
            print(f"âŒ curlæ¨¡æ‹Ÿå¤±è´¥: {str(e)}")
        
        return None
    
    def smart_fetch(self, url: str) -> Optional[str]:
        """æ™ºèƒ½è·å– - å°è¯•å¤šç§æ–¹æ³•"""
        print(f"ğŸ¯ æ™ºèƒ½è·å–: {url}")
        
        # æ–¹æ³•1: requests
        print("\nğŸ“¡ æ–¹æ³•1: ä½¿ç”¨requests...")
        content = self.fetch_with_requests(url)
        if content:
            return content
        
        # æ–¹æ³•2: httpx
        print("\nğŸ”— æ–¹æ³•2: ä½¿ç”¨httpx...")
        content = self.fetch_with_httpx(url)
        if content:
            return content
        
        # æ–¹æ³•3: curlæ¨¡æ‹Ÿ
        print("\nğŸ”§ æ–¹æ³•3: ä½¿ç”¨curlæ¨¡æ‹Ÿ...")
        content = self.fetch_with_curl_simulation(url)
        if content:
            return content
        
        print("\nâŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†")
        return None

def test_enhanced_network():
    """æµ‹è¯•å¢å¼ºç½‘ç»œæ ¸å¿ƒ"""
    print("ğŸš€ æµ‹è¯•å¢å¼ºç½‘ç»œæ ¸å¿ƒ")
    print("=" * 60)
    
    core = EnhancedNetworkCore()
    test_url = "https://missav.ws/ofje-505"
    
    content = core.smart_fetch(test_url)
    
    if content:
        print(f"\nâœ… æˆåŠŸè·å–å†…å®¹!")
        print(f"   é•¿åº¦: {len(content)}")
        
        # æ£€æŸ¥å…³é”®å†…å®¹
        if 'ofje-505' in content.lower():
            print("âœ… å†…å®¹åŒ…å«è§†é¢‘ä»£ç ")
        else:
            print("âš ï¸ å†…å®¹ä¸åŒ…å«è§†é¢‘ä»£ç ")
        
        # ä¿å­˜åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
        debug_file = Path("./debug_content.html")
        debug_file.write_text(content, encoding='utf-8')
        print(f"ğŸ“ å†…å®¹å·²ä¿å­˜åˆ°: {debug_file}")
        
        return True
    else:
        print("\nâŒ è·å–å†…å®¹å¤±è´¥")
        return False

if __name__ == "__main__":
    test_enhanced_network()